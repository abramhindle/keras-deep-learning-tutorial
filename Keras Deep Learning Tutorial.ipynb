{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Tutorial\n",
    "\n",
    "#### Abram Hindle\n",
    "#### <abram.hindle@ualberta.ca>\n",
    "#### http://softwareprocess.ca/\n",
    "\n",
    "Slides stolen gracefully from Ben Zittlau\n",
    "\n",
    "Slide content under CC-BY-SA 4.0 and MIT License for source code or the same license as Python3 or Keras. Slide Source code is MIT License as well.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "### What is machine learning?\n",
    "\n",
    "Building a function from data to classify, predict, group, or represent data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "### Machine Learning\n",
    "\n",
    "There are a few kinds of tasks or functions that could help us here.\n",
    "\n",
    "* Classification: given some input, predict the class that it belongs\n",
    "  to. Given a point is it in the red or in the blue?\n",
    "* Regression: Given a point what will its value be? In the case of a\n",
    "  function with a continuous or numerous discrete outputs it might be\n",
    "  appropriate.\n",
    "* Representation: Learn a smaller representation of the input\n",
    "  data. E.g. we have 300 features lets describe them in a 128-bit hash.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Intro\n",
    "### Motivational Example\n",
    "\n",
    "Imagine we have this data:\n",
    "\n",
    "![2 crescent slices](images/slice.png \"A function we want to learn\n",
    " f(x,y) -> z where z is red\")\n",
    "\n",
    "[See src/genslice.py to see how we made it.](src/genslice.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# purpose: make a slight difference of circles be the dataset to learn.\n",
    "import numpy as np\n",
    "gY, gX = np.meshgrid(np.arange(1000)/1000.0,np.arange(1000)/1000.0)\n",
    "\n",
    "def intersect_circle(cx,cy,radius):\n",
    "    equation = (gX - float(cx)) ** 2 + (gY - float(cy)) ** 2\n",
    "    matches = equation < (radius**3) \n",
    "    return matches\n",
    "\n",
    "# rad = 0.1643167672515498\n",
    "rad = 0.3\n",
    "x = intersect_circle(0.5,0.5,rad) ^ intersect_circle(0.51,0.51,rad)\n",
    "\n",
    "def plotit(x):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.imshow(x)\n",
    "    plt.savefig('new-slice.png') # was slice.png\n",
    "    plt.imshow(x)\n",
    "    plt.savefig('new-slice.pdf') # was slice.pdf\n",
    "    plt.show()\n",
    "\n",
    "# plotit(x)\n",
    "\n",
    "def mkcol(x):\n",
    "    return x.reshape((x.shape[0]*x.shape[1],1))\n",
    "\n",
    "# make the data set\n",
    "big = np.concatenate((mkcol(gX),mkcol(gY),mkcol(1*x)),axis=1)\n",
    "np.savetxt(\"new-big-slice.csv\", big, delimiter=\",\")\n",
    "\n",
    "# make a 50/50 data set\n",
    "nots = big[big[0:,2]==0.0,]\n",
    "np.random.shuffle(nots)\n",
    "nots = nots[0:1000,]\n",
    "trues = big[big[0:,2]==1.0,]\n",
    "np.random.shuffle(trues)\n",
    "trues = trues[0:1000,]\n",
    "small = np.concatenate((trues,nots))\n",
    "np.savetxt(\"new-small-slice.csv\", small, delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "### Make your own function\n",
    "\n",
    "``` python\n",
    "def in_circle(x,y,cx,cy,radius):\n",
    "    return (x - float(cx)) ** 2 + (y - float(cy)) ** 2 < radius**2\n",
    "\n",
    "def mysolution(pt,outer=0.3):\n",
    "    return in_circle(pt[0],pt[1],0.5,0.5,outer) and not in_circle(pt[0],pt[1],0.5,0.5,0.1)\n",
    "```\n",
    "\n",
    "```\n",
    ">>> myclasses = np.apply_along_axis(mysolution,1,test[0])\n",
    ">>> print \"My classifier!\"\n",
    "My classifier!\n",
    ">>> print \"%s / %s \" % (sum(myclasses == test[1]),len(test[1]))\n",
    "181 / 200 \n",
    ">>> print theautil.classifications(myclasses,test[1])\n",
    "[('tp', 91), ('tn', 90), ('fp', 19), ('fn', 0)]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Intro \n",
    "### An example classifier\n",
    "\n",
    "1-NN: 1 Nearest Neighbor.\n",
    "\n",
    "Given the data, we produce a function that\n",
    "outputs the CLASS of the nearest neighbour to the input data.\n",
    "\n",
    "Whoever is closer, is the class. 3-NN is 3-nearest neighbors whereby\n",
    "we use voting of the 3 neighbors instead.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "### An example classifier: 1-NN\n",
    "\n",
    "[src/slice-classifier.py](src/slice-classifier.py)\n",
    "\n",
    "``` python\n",
    "def euclid(pt1,pt2):\n",
    "    return sum([ (pt1[i] - pt2[i])**2 for i in range(0,len(pt1)) ])\n",
    "\n",
    "def oneNN(data,labels):\n",
    "    def func(input):\n",
    "        distance = None\n",
    "        label = None\n",
    "        for i in range(0,len(data)):\n",
    "            d = euclid(input,data[i])\n",
    "            if distance == None or d < distance:\n",
    "                distance = d\n",
    "                label = labels[i]\n",
    "        return label\n",
    "    return func\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Intro\n",
    "### An example classifier: 1-NN\n",
    "\n",
    "``` python\n",
    ">>> learner = oneNN(train[0],train[1])\n",
    ">>> \n",
    ">>> oneclasses = np.apply_along_axis(learner,1,test[0])\n",
    ">>> print \"1-NN classifier!\"\n",
    "1-NN classifier!\n",
    ">>> print \"%s / %s \" % (sum(oneclasses == test[1]),len(test[1]))\n",
    "198 / 200 \n",
    ">>> print theautil.classifications(oneclasses,test[1])\n",
    "[('tp', 91), ('tn', 107), ('fp', 2), ('fn', 0)]\n",
    "\n",
    "```\n",
    "\n",
    "1-NN has great performance in this example, but it uses Euclidean\n",
    "distance and the dataset is really quite biased to the positive\n",
    "classes.\n",
    "\n",
    "Thus we showed a simple learner that classifies data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Intro\n",
    "\n",
    "* That's really interesting performance and it worked but will it\n",
    "  scale and continue to work?\n",
    "\n",
    "* 1-NN doesn't work for all problems. And it is dependent on linear\n",
    "  relationships.\n",
    "\n",
    "* What if our problem is non-linear?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 2 Valid: 2\n",
      "Train X: (1620, 2) Y^:(1620, 1)\n",
      "Valid X: (180, 2)  Y^:(180, 1)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# The MIT License (MIT)\n",
    "# \n",
    "# Copyright (c) 2016 Abram Hindle <hindle1@ualberta.ca>, Leif Johnson <leif@lmjohns3.com>\n",
    "# \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "# \n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "# \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE.\n",
    "\n",
    "# first off we load up some modules we want to use\n",
    "# import keras\n",
    "import tensorflow.keras as keras\n",
    "import scipy\n",
    "import math\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import logging\n",
    "import sys\n",
    "import collections\n",
    "import theautil\n",
    "\n",
    "# setup logging\n",
    "logging.basicConfig(stream = sys.stderr, level=logging.INFO)\n",
    "\n",
    "mupdates = 1000\n",
    "data = np.loadtxt(\"small-slice.csv\", delimiter=\",\")\n",
    "inputs  = data[0:,0:2].astype(np.float32)\n",
    "outputs = data[0:,2:3].astype(np.int32)\n",
    "\n",
    "theautil.joint_shuffle(inputs,outputs)\n",
    "\n",
    "train_and_valid, test = theautil.split_validation(90, inputs, outputs)\n",
    "train, valid = theautil.split_validation(90, train_and_valid[0], train_and_valid[1])\n",
    "print(\"Train: %s Valid: %s\"%(len(train),len(valid)))\n",
    "print(\"Train X: %s Y^:%s\"%(train[0].shape,train[1].shape))\n",
    "print(\"Valid X: %s  Y^:%s\"%(valid[0].shape,valid[1].shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My classifier!\n",
      "172 / 200 \n",
      "[('tp', 99), ('tn', 73), ('fp', 28), ('fn', 0)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def linit(x):\n",
    "    return x.reshape((len(x),))\n",
    "\n",
    "mltrain = (train[0],linit(train[1]))\n",
    "mlvalid = (valid[0],linit(valid[1]))\n",
    "mltest  = (test[0] ,linit(test[1]))\n",
    "\n",
    "# my solution\n",
    "def in_circle(x,y,cx,cy,radius):\n",
    "    return (x - float(cx)) ** 2 + (y - float(cy)) ** 2 < radius**2\n",
    "\n",
    "def mysolution(pt,outer=0.3):\n",
    "    return in_circle(pt[0],pt[1],0.5,0.5,outer) and \\\n",
    "           not in_circle(pt[0],pt[1],0.5,0.5,0.1)\n",
    "\n",
    "# apply my classifier\n",
    "myclasses = np.apply_along_axis(mysolution,1,mltest[0])\n",
    "print(\"My classifier!\")\n",
    "print(\"%s / %s \" % (sum(myclasses == mltest[1]),len(mltest[1])))\n",
    "print(theautil.classifications(myclasses,mltest[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-NN classifier!\n",
      "191 / 200 \n",
      "[('tp', 97), ('tn', 94), ('fp', 7), ('fn', 2)]\n",
      "1-NN classifier! ON TRAIN\n",
      "1620 / 1620 \n",
      "[('tp', 805), ('tn', 815), ('fp', 0), ('fn', 0)]\n"
     ]
    }
   ],
   "source": [
    "def euclid(pt1,pt2):\n",
    "    return sum([ (pt1[i] - pt2[i])**2 for i in range(0,len(pt1)) ])\n",
    "\n",
    "def oneNN(data,labels):\n",
    "    def func(input):\n",
    "        distance = None\n",
    "        label = None\n",
    "        for i in range(0,len(data)):\n",
    "            d = euclid(input,data[i])\n",
    "            if distance == None or d < distance:\n",
    "                distance = d\n",
    "                label = labels[i]\n",
    "        return label\n",
    "    return func\n",
    "\n",
    "learner = oneNN(mltrain[0],mltrain[1])\n",
    "\n",
    "oneclasses = np.apply_along_axis(learner,1,mltest[0])\n",
    "print(\"1-NN classifier!\")\n",
    "print(\"%s / %s \" % (sum(oneclasses == mltest[1]),len(mltest[1])))\n",
    "print(theautil.classifications(oneclasses,mltest[1]))\n",
    "\n",
    "toneclasses = np.apply_along_axis(learner,1,mltrain[0])\n",
    "print(\"1-NN classifier! ON TRAIN\")\n",
    "print(\"%s / %s \" % (sum(toneclasses == mltrain[1]),len(mltrain[1])))\n",
    "print(theautil.classifications(toneclasses,mltrain[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Intro\n",
    "\n",
    "* Neural networks are popular\n",
    "   * Creating AI for Go\n",
    "   * Labeling Images with cats and dogs\n",
    "   * Speech Recognition\n",
    "   * Text summarization\n",
    "   * [Guitar Transcription](https://peerj.com/preprints/1193.pdf)\n",
    "   * Learn audio from video[1](https://archive.org/details/DeepLearningBitmaptoPCM/)[2](http://softwareprocess.es/blog/blog/2015/08/10/deep-learning-bitmaps-to-pcm/)\n",
    "\n",
    "* Neural networks can not only classify, but they can create content,\n",
    "  they can have complicated outputs.\n",
    "\n",
    "* Neural networks are generative!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "### Machine Learning: Neural Networks\n",
    "\n",
    "Neural networks or \"Artificial Neural Networks\" are a flexible class\n",
    "of non-linear machine learners. They have been found to be quite\n",
    "effective as of late.\n",
    "\n",
    "Neural networks are composed of neurons. These neurons try to emulate\n",
    "biological neurons in the most metaphorical of senses. Given a set of\n",
    "inputs they produce an output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Neurons\n",
    "\n",
    "Neurons have functions.\n",
    "\n",
    "* Rectified Linear Units have been shown to train quite well and\n",
    "  achieve good results. By they aren't easier to differentiate.\n",
    "  f(x) = max(0,x)\n",
    "* Sigmoid functions are slow and were the classical neural network\n",
    "  neuron, but have fallen out of favour. They will work when nothing\n",
    "  else will. f(x) = 1/(1 + e^-x)\n",
    "* Softplus is a RELU that is slower to compute but differentiable.\n",
    "  f(x) = ln(1 + e^x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Neurons\n",
    "\n",
    "![Rectifier and Sigmoid and Softplus](images/Rectifier_and_softplus_functions.svg)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Neurons\n",
    "\n",
    "The inputs to a neural network? The outputs of connected nodes times\n",
    "their weight + a bias.\n",
    "\n",
    "neuron(inputs) = neuron_f( sum(weights * inputs) + bias  )\n",
    "\n",
    "![Neuron example](images/neuron.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5404427087207618, [0.5370495669980353, 0.7162978701990245, 0.7615941559557649, 0.8617231593133063], array([0.4, 0.3, 0.6]))\n",
      "Better 0 (1.127599404013806, [-0.9275690960029245, -0.8151164992414622, 0.3105350983844714, 0.6749277587202853], array([ 1.96182845,  0.49860143, -1.64069095]))\n",
      "Better 1 (0.908874393066306, [0.2773532072151096, 0.9629884273070239, -0.6372304140267427, 0.7385069822328632], array([-1.03830914,  1.70068391,  0.28481242]))\n",
      "Better 3 (0.640419152573612, [0.8746152000981374, 0.9277068371766948, 0.8674933801550946, 0.9234775091987835], array([-0.02952576,  0.28929278,  1.35238564]))\n",
      "Better 8 (0.6189844139229838, [0.7304285708440027, 0.96543124397559, 0.990061448903098, 0.9988729388276196], array([1.72010428, 1.09061285, 0.92964549]))\n",
      "Better 26 (0.6031102155981789, [0.7414851365803927, 0.871737314734937, 0.8583388626614532, 0.9320130506610025], array([0.33323004, 0.38650079, 0.95377018]))\n",
      "Better 32 (0.5817828527342772, [0.8240937654450847, 0.5083383306490641, 0.8808065678375334, 0.6471602161114811], array([ 0.20991043, -0.60895721,  1.16944374]))\n",
      "Better 63 (0.5651977793559514, [0.3668505686885595, 0.5819978366250181, 0.9716138222152295, 0.9837088978621072], array([1.73557365, 0.28069956, 0.38477903]))\n",
      "Better 104 (0.5125394734539612, [0.16189961255716348, 0.9394800187056672, 0.8001496819295387, 0.9904405812965079], array([0.93569138, 1.57026398, 0.16333683]))\n",
      "Better 251 (0.5066572143886138, [-0.08913990974779072, 0.8451624973653162, 0.9372321107130617, 0.995462579864553], array([ 1.80416307,  1.32835036, -0.08937714]))\n",
      "Better 779 (0.5021259845535604, [0.5306049447882859, 0.5460084397740935, 0.4491188500928511, 0.4662639080417593], array([-0.10739085,  0.02168975,  0.59098679]))\n",
      "Better 1159 (0.5007873926351497, [-0.020486068930069642, 0.9478987430745307, 0.946992303606514, 0.998603031549014], array([ 1.8222909 ,  1.83114765, -0.02048894]))\n",
      "Best result!\n",
      "(0.5007873926351497, [-0.020486068930069642, 0.9478987430745307, 0.946992303606514, 0.998603031549014], array([ 1.8222909 ,  1.83114765, -0.02048894]))\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "# this is a neuron\n",
    "def tanh(weights,inputs,bias=0):\n",
    "    return numpy.tanh(numpy.sum(weights * inputs) + bias)\n",
    "# XOR\n",
    "train_x = numpy.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "train_y = numpy.array([0,1,1,0])\n",
    "def rms(train_y_hat, train_y):\n",
    "    return numpy.sqrt(numpy.mean((train_y_hat - train_y)**2))\n",
    "best = None\n",
    "weights = numpy.array([0.4,0.3,0.6])\n",
    "train_y_hat = [tanh(weights[0:2],t,weights[2]) for t in train_x]\n",
    "res = (rms(train_y_hat, train_y), train_y_hat, weights)\n",
    "print(res)\n",
    "\n",
    "for i in range(0,10000):\n",
    "    weights = 4*numpy.random.rand(3)-2\n",
    "    train_y_hat = [tanh(weights[0:2],t,weights[2]) for t in train_x]\n",
    "    res = (rms(train_y_hat, train_y), train_y_hat, weights)\n",
    "    #print(res)\n",
    "    if best is None or res[0] < best[0]:\n",
    "        print(f\"Better {i}\",res)\n",
    "        best = res\n",
    "print(\"Best result!\")\n",
    "print(best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better 0 (0.5592870393120526, [0.948929180147032, 0.9181889521315967, 0.8374994698961262, 0.56359729470419], array([-1.13134382, -0.68796213,  1.23085186,  0.92200869, -1.45184099,\n",
      "       -1.40181833, -1.7942933 , -1.5262489 , -0.40033685]))\n",
      "Better 33 (0.4553915902935687, [-0.06282289145866614, 0.8104013445425384, 0.8775143895410108, 0.880130053701387], array([ 1.98295024,  1.10702613,  0.23319811,  1.99197014,  0.5621918 ,\n",
      "        1.69964449,  1.39790747, -0.8021815 ,  0.19066106]))\n",
      "Better 192 (0.44879836074303975, [0.06502039222574339, 0.8360714429498957, 0.861719216120145, 0.8691709130029678], array([ 0.10963926,  1.60692647, -0.65307161,  1.2370941 ,  0.86618679,\n",
      "       -1.85371591, -0.84706007,  1.60612823,  1.88234969]))\n",
      "Better 255 (0.43490997156889444, [0.650935125860052, 0.8178261208853632, 0.5493715109660263, 0.3108324919326616], array([-1.37373581, -1.09107212, -0.36398479, -0.80348518, -1.08459339,\n",
      "       -0.25738395,  1.719738  ,  1.68637546, -1.08480694]))\n",
      "Better 285 (0.43221337398622856, [0.21176006458276894, 0.5281625757451055, 0.8711386569216124, 0.6805552710769937], array([-1.48292698, -1.84382706, -0.22768863, -0.61639843,  1.09236331,\n",
      "       -0.68067893, -1.60624004,  1.51975429,  1.4791715 ]))\n",
      "Better 1310 (0.4053709429453748, [0.14437740273889482, 0.44888502768540217, 0.7087735396520068, 0.49791265017060876], array([ 1.77418797, -1.91338072, -0.44591608,  0.78777337, -0.21284473,\n",
      "        0.55165534, -0.23903216,  1.45520186,  0.81646827]))\n",
      "Better 1675 (0.3620216803692175, [0.5772639275277156, 0.6896050214933027, 0.8691427701165408, 0.2784537464458654], array([ 1.20213441, -1.88649789, -1.1698083 ,  1.22229931, -1.50819936,\n",
      "        0.96914022, -0.15346578,  0.4828205 ,  1.73930178]))\n",
      "Better 4590 (0.3033588963964545, [0.3494964574806382, 0.704591350614958, 0.7911515863736543, 0.3392267328708749], array([-0.79474347,  0.93466587, -1.02150014,  1.07298108, -1.16621005,\n",
      "        1.39169561,  0.97394653, -0.95772427,  1.91028543]))\n",
      "Better 43812 (0.28682147600921315, [0.1307455703057988, 0.6859416751118442, 0.6165908245083987, -0.2575589349316425], array([ 1.19196563,  1.05763641, -1.72291635, -1.06646535, -1.7405692 ,\n",
      "       -1.96869202, -0.33414358, -1.65476478, -1.40230341]))\n",
      "Better 52273 (0.1541909868462043, [0.07911971539399112, 0.8695225831513671, 0.8915384563509197, 0.24505356470813877], array([-1.34858632,  1.20631396,  0.26331078, -1.61154106, -1.70877228,\n",
      "        1.6263725 , -1.09314704,  1.77827952,  1.91318888]))\n",
      "Best result!\n",
      "(0.1541909868462043, [0.07911971539399112, 0.8695225831513671, 0.8915384563509197, 0.24505356470813877], array([-1.34858632,  1.20631396,  0.26331078, -1.61154106, -1.70877228,\n",
      "        1.6263725 , -1.09314704,  1.77827952,  1.91318888]))\n"
     ]
    }
   ],
   "source": [
    "# now with 3 neurons\n",
    "\n",
    "#  input1--+->neuron1\\\n",
    "#        \\ /         \\+neuron3--->result\n",
    "#        /\\          /\n",
    "#  input2-+->neuron2/\n",
    "\n",
    "import numpy\n",
    "# this is a neuron\n",
    "def tanh(weights,inputs,bias=0):\n",
    "    return numpy.tanh(numpy.sum(weights * inputs) + bias)\n",
    "# XOR\n",
    "train_x = numpy.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "train_y = numpy.array([0,1,1,0])\n",
    "def rms(train_y_hat, train_y):\n",
    "    return numpy.sqrt(numpy.mean((train_y_hat - train_y)**2))\n",
    "best = None\n",
    "\n",
    "def random_weights():\n",
    "    return 4*numpy.random.rand(9)-2\n",
    "#  input1--+->neuron1\\\n",
    "#        \\ /         \\+neuron3--->result\n",
    "#        /\\          /\n",
    "#  input2-+->neuron2/\n",
    "def network(weights,t):\n",
    "    inputs = [tanh(weights[0:2],t,weights[2]), #neuron1\n",
    "              tanh(weights[4:6],t,weights[6])] #neuron2\n",
    "    w = numpy.array([weights[3],weights[7]]) #weights for neuron3\n",
    "    return tanh(w,inputs,weights[8]) #neuron3\n",
    "    \n",
    "\n",
    "for i in range(0,100000):\n",
    "    weights = random_weights()\n",
    "    train_y_hat = [network(weights,t) for t in train_x]\n",
    "    res = (rms(train_y_hat, train_y), train_y_hat, weights)\n",
    "    #print(res)\n",
    "    if best is None or res[0] < best[0]:\n",
    "        print(f\"Better {i}\",res)\n",
    "        best = res\n",
    "print(\"Best result!\")\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Multi-layer perceptron\n",
    "\n",
    "Single hidden layer neural network.\n",
    "\n",
    "![Multi-layer perceptron](images/20160208141015.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "There's nothing particularly crazy about deep learning other than it has more hidden layers.\n",
    "\n",
    "These hidden layers allow it to compute state and address the intricacies of complex functions. But each hidden layer adds a lot of search space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Deep Learning\n",
    "\n",
    "![Deep network, multiple layers](images/20160208141143.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Search\n",
    "\n",
    "How do we find the different weights?\n",
    "\n",
    "Well we need to search a large space. A 2x3x2 network will have 2*3*2\n",
    "weights + 5 biases (3 hidden, 2 output) resulting in 17\n",
    "parameters. That's already a large search space.\n",
    "\n",
    "Most search algorithms measure their error at a certain point\n",
    "(difference between prediction and actual) and then choose a direction\n",
    "in their search space to travel. They do this different ways.\n",
    "One way is by sampling points\n",
    "around themselves in order to compute a gradient or slope and then\n",
    "follow the slope around. The most common way is to calculate the gradient symbolicly, compute the derivatives, and avoid sampling altogether (like stochastic gradient descent).\n",
    "\n",
    "Here's a 3D demo of different search algorithms.\n",
    "\n",
    "[Different Search Parameters](http://www.robertsdionne.com/bouncingball/)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Let's deep learn on our problem\n",
    "\n",
    "![2 crescent slices](images/slice.png \"A function we want to learn\n",
    " f(x,y) -> z where z is red\")\n",
    "\n",
    "Please open [slice-classifier](./src/slice-classifier.py) and a python\n",
    "interpreter such as bpython. Search for Part 3 around line 100.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "########################################################################\n",
      "# Part 3. Let's start using neural networks!\n",
      "########################################################################\n",
      "\n",
      "(1620, 2)\n",
      "(1620, 1)\n",
      "(1620, 2)\n",
      "[[0.577 0.337]\n",
      " [0.874 0.502]\n",
      " [0.657 0.446]\n",
      " [0.357 0.43 ]\n",
      " [0.4   0.382]\n",
      " [0.8   0.401]\n",
      " [0.346 0.076]\n",
      " [0.865 0.825]\n",
      " [0.417 0.374]\n",
      " [0.599 0.249]]\n",
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('''\n",
    "########################################################################\n",
    "# Part 3. Let's start using neural networks!\n",
    "########################################################################\n",
    "''')\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow import convert_to_tensor as tft\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore',sparse=False)\n",
    "enc.fit(train[1])\n",
    "train_y = enc.transform(train[1]).astype('float64')\n",
    "valid_y = enc.transform(valid[1]).astype('float64')\n",
    "test_y = enc.transform(test[1]).astype('float64')\n",
    "\n",
    "print(train[0].shape)\n",
    "print(train[1].shape)\n",
    "print(train_y.shape)\n",
    "\n",
    "print(train[0][0:10])\n",
    "print(train[1][0:10])\n",
    "print(train_y[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_18 (Dense)            (None, 16)                48        \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 32)                544       \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 658\n",
      "Trainable params: 658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.6970 - accuracy: 0.5099 - val_loss: 0.7020 - val_accuracy: 0.4667\n",
      "Epoch 2/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.6971 - accuracy: 0.4957 - val_loss: 0.7111 - val_accuracy: 0.4667\n",
      "Epoch 3/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.6969 - accuracy: 0.4944 - val_loss: 0.6909 - val_accuracy: 0.5333\n",
      "Epoch 4/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.6957 - accuracy: 0.4975 - val_loss: 0.6975 - val_accuracy: 0.4667\n",
      "Epoch 5/100\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.6971 - accuracy: 0.4716 - val_loss: 0.6942 - val_accuracy: 0.4667\n",
      "Epoch 6/100\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.6968 - accuracy: 0.4846 - val_loss: 0.6922 - val_accuracy: 0.5333\n",
      "Epoch 7/100\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.6951 - accuracy: 0.5062 - val_loss: 0.6912 - val_accuracy: 0.5333\n",
      "Epoch 8/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.6948 - accuracy: 0.5198 - val_loss: 0.6941 - val_accuracy: 0.4667\n",
      "Epoch 9/100\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.6960 - accuracy: 0.4994 - val_loss: 0.7235 - val_accuracy: 0.4667\n",
      "Epoch 10/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.6978 - accuracy: 0.4846 - val_loss: 0.6917 - val_accuracy: 0.5333\n",
      "Epoch 11/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.6962 - accuracy: 0.4988 - val_loss: 0.6904 - val_accuracy: 0.5333\n",
      "Epoch 12/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.6958 - accuracy: 0.4994 - val_loss: 0.6990 - val_accuracy: 0.4667\n",
      "Epoch 13/100\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.6963 - accuracy: 0.4735 - val_loss: 0.7106 - val_accuracy: 0.4667\n",
      "Epoch 14/100\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.6951 - accuracy: 0.5105 - val_loss: 0.6940 - val_accuracy: 0.4667\n",
      "Epoch 15/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.6970 - accuracy: 0.4883 - val_loss: 0.6987 - val_accuracy: 0.4667\n",
      "Epoch 16/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.6946 - accuracy: 0.5173 - val_loss: 0.6988 - val_accuracy: 0.4667\n",
      "Epoch 17/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.6961 - accuracy: 0.4827 - val_loss: 0.6893 - val_accuracy: 0.5333\n",
      "Epoch 18/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.6947 - accuracy: 0.5062 - val_loss: 0.6893 - val_accuracy: 0.5333\n",
      "Epoch 19/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.6963 - accuracy: 0.4784 - val_loss: 0.6950 - val_accuracy: 0.4667\n",
      "Epoch 20/100\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.6963 - accuracy: 0.4815 - val_loss: 0.6915 - val_accuracy: 0.4667\n",
      "Epoch 21/100\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.6957 - accuracy: 0.5056 - val_loss: 0.6929 - val_accuracy: 0.4500\n",
      "Epoch 22/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.6953 - accuracy: 0.5117 - val_loss: 0.6953 - val_accuracy: 0.4667\n",
      "Epoch 23/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.6947 - accuracy: 0.5130 - val_loss: 0.6879 - val_accuracy: 0.5500\n",
      "Epoch 24/100\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.6953 - accuracy: 0.4951 - val_loss: 0.6898 - val_accuracy: 0.5556\n",
      "Epoch 25/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.5136 - val_loss: 0.6887 - val_accuracy: 0.5333\n",
      "Epoch 26/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.6960 - accuracy: 0.5086 - val_loss: 0.7091 - val_accuracy: 0.4667\n",
      "Epoch 27/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.6949 - accuracy: 0.4963 - val_loss: 0.6916 - val_accuracy: 0.4333\n",
      "Epoch 28/100\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.6940 - accuracy: 0.5086 - val_loss: 0.6866 - val_accuracy: 0.5333\n",
      "Epoch 29/100\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.6948 - accuracy: 0.4815 - val_loss: 0.6852 - val_accuracy: 0.5833\n",
      "Epoch 30/100\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.6954 - accuracy: 0.4901 - val_loss: 0.6864 - val_accuracy: 0.5444\n",
      "Epoch 31/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.6945 - accuracy: 0.4932 - val_loss: 0.6921 - val_accuracy: 0.4500\n",
      "Epoch 32/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.6903 - accuracy: 0.5272 - val_loss: 0.6874 - val_accuracy: 0.5278\n",
      "Epoch 33/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.6925 - accuracy: 0.5191 - val_loss: 0.6893 - val_accuracy: 0.4056\n",
      "Epoch 34/100\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.6915 - accuracy: 0.5167 - val_loss: 0.6849 - val_accuracy: 0.5500\n",
      "Epoch 35/100\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.6916 - accuracy: 0.5284 - val_loss: 0.6972 - val_accuracy: 0.4667\n",
      "Epoch 36/100\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.6925 - accuracy: 0.4778 - val_loss: 0.6805 - val_accuracy: 0.7000\n",
      "Epoch 37/100\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.6885 - accuracy: 0.5315 - val_loss: 0.6795 - val_accuracy: 0.5833\n",
      "Epoch 38/100\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.6915 - accuracy: 0.5148 - val_loss: 0.6787 - val_accuracy: 0.5889\n",
      "Epoch 39/100\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.6900 - accuracy: 0.5185 - val_loss: 0.6824 - val_accuracy: 0.5500\n",
      "Epoch 40/100\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.6911 - accuracy: 0.5302 - val_loss: 0.6858 - val_accuracy: 0.4056\n",
      "Epoch 41/100\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.6887 - accuracy: 0.4747 - val_loss: 0.6795 - val_accuracy: 0.5556\n",
      "Epoch 42/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.6862 - accuracy: 0.5401 - val_loss: 0.6845 - val_accuracy: 0.4056\n",
      "Epoch 43/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.6857 - accuracy: 0.5043 - val_loss: 0.6722 - val_accuracy: 0.7056\n",
      "Epoch 44/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.6858 - accuracy: 0.5309 - val_loss: 0.6761 - val_accuracy: 0.5611\n",
      "Epoch 45/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.6836 - accuracy: 0.5148 - val_loss: 0.6689 - val_accuracy: 0.7056\n",
      "Epoch 46/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.6810 - accuracy: 0.5259 - val_loss: 0.6756 - val_accuracy: 0.5222\n",
      "Epoch 47/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.6785 - accuracy: 0.5241 - val_loss: 0.6650 - val_accuracy: 0.6611\n",
      "Epoch 48/100\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.6791 - accuracy: 0.5278 - val_loss: 0.6633 - val_accuracy: 0.6444\n",
      "Epoch 49/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.6761 - accuracy: 0.5463 - val_loss: 0.6604 - val_accuracy: 0.7000\n",
      "Epoch 50/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.6740 - accuracy: 0.5444 - val_loss: 0.6574 - val_accuracy: 0.7222\n",
      "Epoch 51/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.6684 - accuracy: 0.5679 - val_loss: 0.6715 - val_accuracy: 0.4667\n",
      "Epoch 52/100\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.6644 - accuracy: 0.5525 - val_loss: 0.6506 - val_accuracy: 0.6778\n",
      "Epoch 53/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.6625 - accuracy: 0.5784 - val_loss: 0.6506 - val_accuracy: 0.6056\n",
      "Epoch 54/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.6582 - accuracy: 0.5914 - val_loss: 0.6436 - val_accuracy: 0.6278\n",
      "Epoch 55/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.6525 - accuracy: 0.6068 - val_loss: 0.6359 - val_accuracy: 0.7333\n",
      "Epoch 56/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.6451 - accuracy: 0.6438 - val_loss: 0.6312 - val_accuracy: 0.7667\n",
      "Epoch 57/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.6369 - accuracy: 0.6611 - val_loss: 0.6469 - val_accuracy: 0.4667\n",
      "Epoch 58/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.6313 - accuracy: 0.6636 - val_loss: 0.6213 - val_accuracy: 0.7444\n",
      "Epoch 59/100\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.6223 - accuracy: 0.6901 - val_loss: 0.6249 - val_accuracy: 0.5556\n",
      "Epoch 60/100\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.6123 - accuracy: 0.7185 - val_loss: 0.6032 - val_accuracy: 0.8333\n",
      "Epoch 61/100\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.6007 - accuracy: 0.7278 - val_loss: 0.5893 - val_accuracy: 0.7667\n",
      "Epoch 62/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.5905 - accuracy: 0.7642 - val_loss: 0.5851 - val_accuracy: 0.8667\n",
      "Epoch 63/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.5789 - accuracy: 0.7772 - val_loss: 0.5674 - val_accuracy: 0.8056\n",
      "Epoch 64/100\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.5628 - accuracy: 0.8049 - val_loss: 0.5564 - val_accuracy: 0.8278\n",
      "Epoch 65/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.5477 - accuracy: 0.8253 - val_loss: 0.5518 - val_accuracy: 0.9056\n",
      "Epoch 66/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.5325 - accuracy: 0.8340 - val_loss: 0.5264 - val_accuracy: 0.8167\n",
      "Epoch 67/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.5169 - accuracy: 0.8327 - val_loss: 0.5145 - val_accuracy: 0.8722\n",
      "Epoch 68/100\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.4949 - accuracy: 0.8475 - val_loss: 0.5275 - val_accuracy: 0.8833\n",
      "Epoch 69/100\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.4823 - accuracy: 0.8506 - val_loss: 0.4835 - val_accuracy: 0.8111\n",
      "Epoch 70/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.4679 - accuracy: 0.8562 - val_loss: 0.4690 - val_accuracy: 0.8722\n",
      "Epoch 71/100\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.4489 - accuracy: 0.8630 - val_loss: 0.4602 - val_accuracy: 0.8889\n",
      "Epoch 72/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.4334 - accuracy: 0.8679 - val_loss: 0.4527 - val_accuracy: 0.8944\n",
      "Epoch 73/100\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.4198 - accuracy: 0.8753 - val_loss: 0.4263 - val_accuracy: 0.8833\n",
      "Epoch 74/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.4045 - accuracy: 0.8821 - val_loss: 0.4178 - val_accuracy: 0.8889\n",
      "Epoch 75/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.3946 - accuracy: 0.8833 - val_loss: 0.4038 - val_accuracy: 0.8889\n",
      "Epoch 76/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.3854 - accuracy: 0.8821 - val_loss: 0.3945 - val_accuracy: 0.8889\n",
      "Epoch 77/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.3734 - accuracy: 0.8802 - val_loss: 0.3990 - val_accuracy: 0.8944\n",
      "Epoch 78/100\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.3641 - accuracy: 0.8840 - val_loss: 0.3772 - val_accuracy: 0.8889\n",
      "Epoch 79/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.3551 - accuracy: 0.8907 - val_loss: 0.3692 - val_accuracy: 0.8889\n",
      "Epoch 80/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.3479 - accuracy: 0.8914 - val_loss: 0.3627 - val_accuracy: 0.8889\n",
      "Epoch 81/100\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.3410 - accuracy: 0.8969 - val_loss: 0.3612 - val_accuracy: 0.8778\n",
      "Epoch 82/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.3366 - accuracy: 0.8969 - val_loss: 0.3557 - val_accuracy: 0.8833\n",
      "Epoch 83/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.3281 - accuracy: 0.8963 - val_loss: 0.3469 - val_accuracy: 0.8889\n",
      "Epoch 84/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.3228 - accuracy: 0.8994 - val_loss: 0.3461 - val_accuracy: 0.8833\n",
      "Epoch 85/100\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.3177 - accuracy: 0.8988 - val_loss: 0.3558 - val_accuracy: 0.9056\n",
      "Epoch 86/100\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.3155 - accuracy: 0.9019 - val_loss: 0.3340 - val_accuracy: 0.8889\n",
      "Epoch 87/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.3105 - accuracy: 0.9025 - val_loss: 0.3344 - val_accuracy: 0.8889\n",
      "Epoch 88/100\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.3074 - accuracy: 0.8994 - val_loss: 0.3274 - val_accuracy: 0.8889\n",
      "Epoch 89/100\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.3047 - accuracy: 0.8994 - val_loss: 0.3249 - val_accuracy: 0.8889\n",
      "Epoch 90/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.3015 - accuracy: 0.9006 - val_loss: 0.3222 - val_accuracy: 0.8889\n",
      "Epoch 91/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.3010 - accuracy: 0.9037 - val_loss: 0.3229 - val_accuracy: 0.8889\n",
      "Epoch 92/100\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.2945 - accuracy: 0.9074 - val_loss: 0.3364 - val_accuracy: 0.8833\n",
      "Epoch 93/100\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.2950 - accuracy: 0.9043 - val_loss: 0.3146 - val_accuracy: 0.8944\n",
      "Epoch 94/100\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.2924 - accuracy: 0.9037 - val_loss: 0.3168 - val_accuracy: 0.9056\n",
      "Epoch 95/100\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.2912 - accuracy: 0.9074 - val_loss: 0.3131 - val_accuracy: 0.9056\n",
      "Epoch 96/100\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.2857 - accuracy: 0.9080 - val_loss: 0.3126 - val_accuracy: 0.9000\n",
      "Epoch 97/100\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.2854 - accuracy: 0.9080 - val_loss: 0.3100 - val_accuracy: 0.9000\n",
      "Epoch 98/100\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.2839 - accuracy: 0.9068 - val_loss: 0.3085 - val_accuracy: 0.9000\n",
      "Epoch 99/100\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.2828 - accuracy: 0.9068 - val_loss: 0.3138 - val_accuracy: 0.8889\n",
      "Epoch 100/100\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 0.2802 - accuracy: 0.9099 - val_loss: 0.3172 - val_accuracy: 0.8889\n",
      "CPU times: user 1min 21s, sys: 6.86 s, total: 1min 28s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# rerunning this will produce different results\n",
    "# try different combos here\n",
    "net = Sequential([\n",
    "    Dense(16,input_shape=(2,),activation=\"sigmoid\"),\n",
    "    Dense(32,activation=\"sigmoid\"),\n",
    "    Dense(2,activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# opt = SGD()#\n",
    "opt = Adam() # lr=0.1)\n",
    "net.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "print(net.summary())\n",
    "# net.fit(x=train[0],y=train_y, epochs=100)\n",
    "history = net.fit(x=train[0], y=train_y, \n",
    "                validation_data=(valid[0], valid_y),\n",
    "                epochs=100, batch_size=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learner on the test set\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3320 - accuracy: 0.8650\n",
      "Scores: [0.3319746255874634, 0.8650000095367432]\n",
      "173 / 200 \n",
      "Counter({1: 126, 0: 74})\n",
      "[('tp', 99), ('tn', 74), ('fp', 27), ('fn', 0)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Learner on the test set\")\n",
    "score = net.evaluate(test[0], test_y)\n",
    "print(\"Scores: %s\" % score)\n",
    "predictit = net.predict(test[0])\n",
    "#print(predictit.shape)\n",
    "#print(predictit[0:10,])\n",
    "#print(net.__class__)\n",
    "#classify = net.predict_classes(test[0])\n",
    "#predict_x=net.predict(test[0]) \n",
    "#classify=np.argmax(predict_x,axis=1)\n",
    "\n",
    "def predict_classes(net,test):\n",
    "    predict_x=net.predict(test) \n",
    "    classify=np.argmax(predict_x,axis=1)\n",
    "    return classify\n",
    "    \n",
    "classify = predict_classes(net, test[0])\n",
    "print(\"%s / %s \" % (np.sum(classify == mltest[1]),len(mltest[1])))\n",
    "print(collections.Counter(classify))\n",
    "print(theautil.classifications(classify,mltest[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learner on the test set\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5050\n",
      "Scores: [0.6930779218673706, 0.5049999952316284]\n",
      "(200, 2)\n",
      "[[0.5065479  0.4934521 ]\n",
      " [0.5087744  0.49122557]\n",
      " [0.50861984 0.49138016]\n",
      " [0.5110478  0.4889522 ]\n",
      " [0.50453234 0.49546763]\n",
      " [0.50833464 0.49166536]\n",
      " [0.50873536 0.49126458]\n",
      " [0.50529075 0.49470925]\n",
      " [0.50869983 0.4913002 ]\n",
      " [0.50673586 0.49326417]]\n",
      "0 / 200 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2591890/3972499812.py:9: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  print(\"%s / %s \" % (np.sum(classify == mltest[1]),len(mltest[1])))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2591890/3972499812.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s / %s \"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassify\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmltest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmltest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheautil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifications\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmltest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/collections/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, iterable, **kwds)\u001b[0m\n\u001b[1;32m    550\u001b[0m         '''\n\u001b[1;32m    551\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__missing__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/collections/__init__.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, iterable, **kwds)\u001b[0m\n\u001b[1;32m    635\u001b[0m                     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# fast path when counter is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m                 \u001b[0m_count_elements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And now on more unseen data that isn't 50/50\n",
      "2383 / 3000 \n",
      "Counter({0: 2354, 1: 646})\n",
      "[('tp', 29), ('tn', 2354), ('fp', 617), ('fn', 0)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def real_function(pt):\n",
    "    rad = 0.1643167672515498\n",
    "    in1 = in_circle(pt[0],pt[1],0.5,0.5,rad)\n",
    "    in2 = in_circle(pt[0],pt[1],0.51,0.51,rad)\n",
    "    return in1 ^ in2\n",
    "\n",
    "print(\"And now on more unseen data that isn't 50/50\")\n",
    "\n",
    "bigtest = np.random.uniform(size=(3000,2)).astype(np.float32)\n",
    "biglab = np.apply_along_axis(real_function,1,bigtest).astype(np.int32)\n",
    "\n",
    "classify = predict_classes(net,bigtest)\n",
    "print(\"%s / %s \" % (sum(classify == biglab),len(biglab)))\n",
    "print(collections.Counter(classify))\n",
    "print(theautil.classifications(classify,biglab))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Now let's discuss posing problems for neural networks\n",
    "\n",
    "* Scaling inputs: Scaling can sometimes help, so can\n",
    "  standardization. This means constraining values or re-centering\n",
    "  them. It depends on your problem and it is worth trying.\n",
    "\n",
    "* E.g. min max scaling:\n",
    "\n",
    "``` python\n",
    "def min_max_scale(data):\n",
    "    '''scales data by minimum and maximum values between 0 and 1'''\n",
    "    dmin = np.min(data)\n",
    "    return (data - dmin)/(np.max(data) - dmin)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem\n",
    "\n",
    "* [posing.py](src/posing.py) tries to show the problem of taking\n",
    "  random input data and determine what distribution it comes from.\n",
    "  That is what function can produce these random values.\n",
    "\n",
    "* Let's open up [posing.py](src/posing.py) and get an interpreter\n",
    "  going.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration of how to pose the problem and how different formulations\n",
    "# lead to different results!\n",
    "#\n",
    "# The MIT License (MIT)\n",
    "# \n",
    "# Copyright (c) 2016 Abram Hindle <hindle1@ualberta.ca>, Leif Johnson <leif@lmjohns3.com>\n",
    "# \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "# \n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "# \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE.\n",
    "\n",
    "# first off we load up some modules we want to use\n",
    "import keras\n",
    "import scipy\n",
    "import math\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import logging\n",
    "import sys\n",
    "from numpy.random import power, normal, lognormal, uniform\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import theautil\n",
    "import tensorflow as tf\n",
    "\n",
    "# What are we going to do?\n",
    "# - we're going to generate data derived from 4 different distributions\n",
    "# - we're going to scale that data\n",
    "# - we're going to create a RBM (1 hidden layer neural network)\n",
    "# - we're going to train it to classify data as belonging to one of these distributions\n",
    "\n",
    "# maximum number of iterations before we bail\n",
    "mupdates = 1000\n",
    "\n",
    "# setup logging\n",
    "logging.basicConfig(stream = sys.stderr, level=logging.INFO)\n",
    "\n",
    "# how we pose our problem to the deep belief network matters.\n",
    "\n",
    "# lets make the task easier by scaling all values between 0 and 1\n",
    "def min_max_scale(data):\n",
    "    '''scales data by minimum and maximum values between 0 and 1'''\n",
    "    dmin = np.min(data)\n",
    "    return (data - dmin)/(np.max(data) - dmin)\n",
    "\n",
    "# how many samples per each distribution\n",
    "bsize    = 100 \n",
    "\n",
    "# poor man's enum\n",
    "LOGNORMAL=0\n",
    "POWER=1\n",
    "NORM=2\n",
    "UNIFORM=3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Experiment 1\n",
    "\n",
    "* Given 1 single sample what distribution does it come from?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "########################################################################\n",
      "# Experiment 1: can we classify single samples?\n",
      "#\n",
      "#\n",
      "#########################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "########################################################################\n",
    "# Experiment 1: can we classify single samples?\n",
    "#\n",
    "#\n",
    "#########################################################################\n",
    "''')\n",
    "\n",
    "def make_dataset1():\n",
    "    '''Make a dataset of single samples with labels from which distribution they come from'''\n",
    "    # now lets make some samples \n",
    "    lns      = min_max_scale(lognormal(size=bsize)) #log normal\n",
    "    powers   = min_max_scale(power(0.1,size=bsize)) #power law\n",
    "    norms    = min_max_scale(normal(size=bsize))    #normal\n",
    "    uniforms = min_max_scale(uniform(size=bsize))    #uniform\n",
    "    # add our data together\n",
    "    data = np.concatenate((lns,powers,norms,uniforms))\n",
    "    \n",
    "    # concatenate our labels\n",
    "    labels = np.concatenate((\n",
    "        (np.repeat(LOGNORMAL,bsize)),\n",
    "        (np.repeat(POWER,bsize)),\n",
    "        (np.repeat(NORM,bsize)),\n",
    "        (np.repeat(UNIFORM,bsize))))\n",
    "    tsize = len(labels)\n",
    "    \n",
    "    # make sure dimensionality and types are right\n",
    "    data = data.reshape((len(data),1))\n",
    "    data = data.astype(np.float32)\n",
    "    labels = labels.astype(np.int32)\n",
    "    labels = labels.reshape((len(data),))\n",
    "    \n",
    "    return data, labels, tsize\n",
    "\n",
    "# this will be the training data and validation data\n",
    "data, labels, tsize = make_dataset1()\n",
    "\n",
    "\n",
    "# this is the test data, this is kept separate to prove we can\n",
    "# actually work on the data we claim we can.\n",
    "#\n",
    "# Without test data, you might just have great performance on the\n",
    "# train set.\n",
    "test_data, test_labels, _ = make_dataset1()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(360, 1)\n",
      "(360,)\n",
      "(360, 4)\n",
      "(40, 4)\n",
      "(400, 4)\n"
     ]
    }
   ],
   "source": [
    "# utilities\n",
    "\n",
    "# now lets shuffle\n",
    "# If we're going to select a validation set we probably want to shuffle\n",
    "def joint_shuffle(arr1,arr2):\n",
    "    assert len(arr1) == len(arr2)\n",
    "    indices = np.arange(len(arr1))\n",
    "    np.random.shuffle(indices)\n",
    "    arr1[0:len(arr1)] = arr1[indices]\n",
    "    arr2[0:len(arr2)] = arr2[indices]\n",
    "\n",
    "# our data and labels are shuffled together\n",
    "joint_shuffle(data,labels)\n",
    "\n",
    "def split_validation(percent, data, labels):\n",
    "    ''' \n",
    "    split_validation splits a dataset of data and labels into\n",
    "    2 partitions at the percent mark\n",
    "    percent should be an int between 1 and 99\n",
    "    '''\n",
    "    s = int(percent * len(data) / 100)\n",
    "    tdata = data[0:s]\n",
    "    vdata = data[s:]\n",
    "    tlabels = labels[0:s]\n",
    "    vlabels = labels[s:]\n",
    "    return ((tdata,tlabels),(vdata,vlabels))\n",
    "\n",
    "# make a validation set from the train set\n",
    "train1, valid1 = split_validation(90, data, labels)\n",
    "\n",
    "print(train1[0].shape)\n",
    "print(train1[1].shape)\n",
    "\n",
    "enc1 = OneHotEncoder(handle_unknown='ignore',sparse=False)\n",
    "enc1.fit(train1[1].reshape(len(train1[1]),1))\n",
    "train1_y = enc1.transform(train1[1].reshape(len(train1[1]),1))\n",
    "print(train1_y.shape)\n",
    "valid1_y = enc1.transform(valid1[1].reshape(len(valid1[1]),1))\n",
    "print(valid1_y.shape)\n",
    "test1_y = enc1.transform(test_labels.reshape(len(test_labels),1))\n",
    "print(test1_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're building a MLP of 1 input layer node, 4 hidden layer nodes, and an output layer of 4 nodes. The output layer has 4 nodes because we have 4 classes that the neural network will output.\n",
      "Epoch 1/100\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1.4480 - accuracy: 0.0972 - val_loss: 1.4242 - val_accuracy: 0.1250\n",
      "Epoch 2/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.3990 - accuracy: 0.1139 - val_loss: 1.4039 - val_accuracy: 0.1750\n",
      "Epoch 3/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.3913 - accuracy: 0.2694 - val_loss: 1.3994 - val_accuracy: 0.3250\n",
      "Epoch 4/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.3863 - accuracy: 0.3528 - val_loss: 1.3903 - val_accuracy: 0.1500\n",
      "Epoch 5/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.3824 - accuracy: 0.3000 - val_loss: 1.3896 - val_accuracy: 0.2250\n",
      "Epoch 6/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.3795 - accuracy: 0.2833 - val_loss: 1.3916 - val_accuracy: 0.3250\n",
      "Epoch 7/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.3769 - accuracy: 0.2333 - val_loss: 1.4004 - val_accuracy: 0.3000\n",
      "Epoch 8/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.3728 - accuracy: 0.3611 - val_loss: 1.3924 - val_accuracy: 0.3000\n",
      "Epoch 9/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.3705 - accuracy: 0.3750 - val_loss: 1.3844 - val_accuracy: 0.3250\n",
      "Epoch 10/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.3660 - accuracy: 0.4028 - val_loss: 1.3782 - val_accuracy: 0.3750\n",
      "Epoch 11/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.3648 - accuracy: 0.3083 - val_loss: 1.3784 - val_accuracy: 0.3000\n",
      "Epoch 12/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.3583 - accuracy: 0.3500 - val_loss: 1.3759 - val_accuracy: 0.3250\n",
      "Epoch 13/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.3548 - accuracy: 0.3889 - val_loss: 1.3758 - val_accuracy: 0.3000\n",
      "Epoch 14/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.3509 - accuracy: 0.3889 - val_loss: 1.3689 - val_accuracy: 0.3000\n",
      "Epoch 15/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.3473 - accuracy: 0.4028 - val_loss: 1.3617 - val_accuracy: 0.3250\n",
      "Epoch 16/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.3426 - accuracy: 0.3639 - val_loss: 1.3627 - val_accuracy: 0.3000\n",
      "Epoch 17/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.3384 - accuracy: 0.3917 - val_loss: 1.3601 - val_accuracy: 0.3000\n",
      "Epoch 18/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.3341 - accuracy: 0.3750 - val_loss: 1.3574 - val_accuracy: 0.3250\n",
      "Epoch 19/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.3296 - accuracy: 0.3972 - val_loss: 1.3512 - val_accuracy: 0.3000\n",
      "Epoch 20/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.3239 - accuracy: 0.3889 - val_loss: 1.3460 - val_accuracy: 0.3750\n",
      "Epoch 21/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1.3192 - accuracy: 0.3889 - val_loss: 1.3440 - val_accuracy: 0.3750\n",
      "Epoch 22/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.3141 - accuracy: 0.4056 - val_loss: 1.3419 - val_accuracy: 0.3250\n",
      "Epoch 23/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.3088 - accuracy: 0.3889 - val_loss: 1.3378 - val_accuracy: 0.3000\n",
      "Epoch 24/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.3037 - accuracy: 0.4028 - val_loss: 1.3316 - val_accuracy: 0.3500\n",
      "Epoch 25/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.2990 - accuracy: 0.3833 - val_loss: 1.3303 - val_accuracy: 0.3250\n",
      "Epoch 26/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.2919 - accuracy: 0.4111 - val_loss: 1.3238 - val_accuracy: 0.3750\n",
      "Epoch 27/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.2887 - accuracy: 0.3944 - val_loss: 1.3181 - val_accuracy: 0.1750\n",
      "Epoch 28/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.2809 - accuracy: 0.3806 - val_loss: 1.3207 - val_accuracy: 0.3000\n",
      "Epoch 29/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1.2762 - accuracy: 0.3778 - val_loss: 1.3167 - val_accuracy: 0.3500\n",
      "Epoch 30/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1.2697 - accuracy: 0.4000 - val_loss: 1.3136 - val_accuracy: 0.4000\n",
      "Epoch 31/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.2640 - accuracy: 0.4139 - val_loss: 1.3107 - val_accuracy: 0.3750\n",
      "Epoch 32/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1.2584 - accuracy: 0.4111 - val_loss: 1.3024 - val_accuracy: 0.3750\n",
      "Epoch 33/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.2516 - accuracy: 0.4056 - val_loss: 1.2952 - val_accuracy: 0.4000\n",
      "Epoch 34/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.2479 - accuracy: 0.3528 - val_loss: 1.2974 - val_accuracy: 0.4250\n",
      "Epoch 35/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1.2421 - accuracy: 0.4000 - val_loss: 1.2958 - val_accuracy: 0.4250\n",
      "Epoch 36/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1.2370 - accuracy: 0.4389 - val_loss: 1.2910 - val_accuracy: 0.3250\n",
      "Epoch 37/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.2304 - accuracy: 0.4111 - val_loss: 1.2882 - val_accuracy: 0.3250\n",
      "Epoch 38/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.2253 - accuracy: 0.4333 - val_loss: 1.2805 - val_accuracy: 0.2000\n",
      "Epoch 39/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.2203 - accuracy: 0.3806 - val_loss: 1.2781 - val_accuracy: 0.3000\n",
      "Epoch 40/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.2157 - accuracy: 0.4306 - val_loss: 1.2741 - val_accuracy: 0.3250\n",
      "Epoch 41/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.2114 - accuracy: 0.3639 - val_loss: 1.2738 - val_accuracy: 0.3250\n",
      "Epoch 42/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.2065 - accuracy: 0.4083 - val_loss: 1.2713 - val_accuracy: 0.3250\n",
      "Epoch 43/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.2021 - accuracy: 0.4139 - val_loss: 1.2690 - val_accuracy: 0.3000\n",
      "Epoch 44/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.1990 - accuracy: 0.3750 - val_loss: 1.2668 - val_accuracy: 0.3000\n",
      "Epoch 45/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.1936 - accuracy: 0.3333 - val_loss: 1.2689 - val_accuracy: 0.4000\n",
      "Epoch 46/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.1891 - accuracy: 0.4250 - val_loss: 1.2610 - val_accuracy: 0.3750\n",
      "Epoch 47/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1.1872 - accuracy: 0.4222 - val_loss: 1.2642 - val_accuracy: 0.3000\n",
      "Epoch 48/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.1824 - accuracy: 0.3917 - val_loss: 1.2600 - val_accuracy: 0.4000\n",
      "Epoch 49/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.1788 - accuracy: 0.4306 - val_loss: 1.2557 - val_accuracy: 0.4000\n",
      "Epoch 50/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.1763 - accuracy: 0.3722 - val_loss: 1.2549 - val_accuracy: 0.4250\n",
      "Epoch 51/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1.1728 - accuracy: 0.3806 - val_loss: 1.2568 - val_accuracy: 0.4000\n",
      "Epoch 52/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1.1705 - accuracy: 0.4278 - val_loss: 1.2571 - val_accuracy: 0.3750\n",
      "Epoch 53/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1.1667 - accuracy: 0.4000 - val_loss: 1.2568 - val_accuracy: 0.4000\n",
      "Epoch 54/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1.1646 - accuracy: 0.4333 - val_loss: 1.2515 - val_accuracy: 0.3750\n",
      "Epoch 55/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.1621 - accuracy: 0.4000 - val_loss: 1.2530 - val_accuracy: 0.3750\n",
      "Epoch 56/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1.1614 - accuracy: 0.4056 - val_loss: 1.2531 - val_accuracy: 0.4000\n",
      "Epoch 57/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1.1583 - accuracy: 0.4361 - val_loss: 1.2539 - val_accuracy: 0.3750\n",
      "Epoch 58/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1.1563 - accuracy: 0.4306 - val_loss: 1.2523 - val_accuracy: 0.3750\n",
      "Epoch 59/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.1542 - accuracy: 0.4222 - val_loss: 1.2500 - val_accuracy: 0.4250\n",
      "Epoch 60/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1.1518 - accuracy: 0.4083 - val_loss: 1.2573 - val_accuracy: 0.3500\n",
      "Epoch 61/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.1514 - accuracy: 0.3917 - val_loss: 1.2596 - val_accuracy: 0.4000\n",
      "Epoch 62/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.1489 - accuracy: 0.4194 - val_loss: 1.2576 - val_accuracy: 0.4000\n",
      "Epoch 63/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.1467 - accuracy: 0.4167 - val_loss: 1.2542 - val_accuracy: 0.4000\n",
      "Epoch 64/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.1457 - accuracy: 0.4167 - val_loss: 1.2502 - val_accuracy: 0.4000\n",
      "Epoch 65/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1.1436 - accuracy: 0.4278 - val_loss: 1.2502 - val_accuracy: 0.3500\n",
      "Epoch 66/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1.1425 - accuracy: 0.4028 - val_loss: 1.2559 - val_accuracy: 0.3000\n",
      "Epoch 67/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1.1427 - accuracy: 0.4222 - val_loss: 1.2477 - val_accuracy: 0.4250\n",
      "Epoch 68/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.1413 - accuracy: 0.3944 - val_loss: 1.2515 - val_accuracy: 0.4000\n",
      "Epoch 69/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.1388 - accuracy: 0.4694 - val_loss: 1.2488 - val_accuracy: 0.3500\n",
      "Epoch 70/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1.1375 - accuracy: 0.4139 - val_loss: 1.2485 - val_accuracy: 0.2500\n",
      "Epoch 71/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.1380 - accuracy: 0.3778 - val_loss: 1.2495 - val_accuracy: 0.4000\n",
      "Epoch 72/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1.1377 - accuracy: 0.4139 - val_loss: 1.2515 - val_accuracy: 0.3250\n",
      "Epoch 73/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.1357 - accuracy: 0.3972 - val_loss: 1.2497 - val_accuracy: 0.4250\n",
      "Epoch 74/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.1353 - accuracy: 0.4056 - val_loss: 1.2480 - val_accuracy: 0.4250\n",
      "Epoch 75/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1.1331 - accuracy: 0.4194 - val_loss: 1.2536 - val_accuracy: 0.3500\n",
      "Epoch 76/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.1314 - accuracy: 0.3889 - val_loss: 1.2534 - val_accuracy: 0.4250\n",
      "Epoch 77/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.1319 - accuracy: 0.4167 - val_loss: 1.2575 - val_accuracy: 0.3750\n",
      "Epoch 78/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.1308 - accuracy: 0.4472 - val_loss: 1.2534 - val_accuracy: 0.3500\n",
      "Epoch 79/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1.1294 - accuracy: 0.3972 - val_loss: 1.2489 - val_accuracy: 0.4250\n",
      "Epoch 80/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1.1288 - accuracy: 0.4167 - val_loss: 1.2505 - val_accuracy: 0.4000\n",
      "Epoch 81/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1.1285 - accuracy: 0.4083 - val_loss: 1.2518 - val_accuracy: 0.4000\n",
      "Epoch 82/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.1282 - accuracy: 0.4139 - val_loss: 1.2546 - val_accuracy: 0.3250\n",
      "Epoch 83/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.1270 - accuracy: 0.3889 - val_loss: 1.2565 - val_accuracy: 0.3000\n",
      "Epoch 84/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.1255 - accuracy: 0.4139 - val_loss: 1.2535 - val_accuracy: 0.3500\n",
      "Epoch 85/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.1270 - accuracy: 0.3972 - val_loss: 1.2537 - val_accuracy: 0.4000\n",
      "Epoch 86/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.1244 - accuracy: 0.3806 - val_loss: 1.2536 - val_accuracy: 0.4250\n",
      "Epoch 87/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.1233 - accuracy: 0.4167 - val_loss: 1.2497 - val_accuracy: 0.4000\n",
      "Epoch 88/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.1238 - accuracy: 0.4389 - val_loss: 1.2506 - val_accuracy: 0.4250\n",
      "Epoch 89/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.1238 - accuracy: 0.4167 - val_loss: 1.2518 - val_accuracy: 0.4000\n",
      "Epoch 90/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.1222 - accuracy: 0.4417 - val_loss: 1.2563 - val_accuracy: 0.3750\n",
      "Epoch 91/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.1231 - accuracy: 0.3611 - val_loss: 1.2544 - val_accuracy: 0.4000\n",
      "Epoch 92/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.1217 - accuracy: 0.4222 - val_loss: 1.2504 - val_accuracy: 0.4500\n",
      "Epoch 93/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.1213 - accuracy: 0.4194 - val_loss: 1.2515 - val_accuracy: 0.3750\n",
      "Epoch 94/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.1195 - accuracy: 0.4222 - val_loss: 1.2502 - val_accuracy: 0.4500\n",
      "Epoch 95/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.1187 - accuracy: 0.4139 - val_loss: 1.2557 - val_accuracy: 0.3500\n",
      "Epoch 96/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.1197 - accuracy: 0.4250 - val_loss: 1.2553 - val_accuracy: 0.4250\n",
      "Epoch 97/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.1183 - accuracy: 0.4472 - val_loss: 1.2528 - val_accuracy: 0.4250\n",
      "Epoch 98/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.1191 - accuracy: 0.4444 - val_loss: 1.2538 - val_accuracy: 0.3250\n",
      "Epoch 99/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.1181 - accuracy: 0.3917 - val_loss: 1.2554 - val_accuracy: 0.3750\n",
      "Epoch 100/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.1187 - accuracy: 0.4278 - val_loss: 1.2543 - val_accuracy: 0.4000\n",
      "[('tp', 0), ('tn', 89), ('fp', 0), ('fn', 84)]\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.1100 - accuracy: 0.4400\n",
      "Scores: [1.1100176572799683, 0.4399999976158142]\n"
     ]
    }
   ],
   "source": [
    "# build our classifier\n",
    "\n",
    "print(\"We're building a MLP of 1 input layer node, 4 hidden layer nodes, and an output layer of 4 nodes. The output layer has 4 nodes because we have 4 classes that the neural network will output.\")\n",
    "cnet = Sequential()\n",
    "cnet.add(Dense(4,input_shape=(1,),activation=\"sigmoid\"))\n",
    "cnet.add(Dense(4,activation=\"softmax\"))\n",
    "copt = SGD(lr=0.1)\n",
    "# opt = Adam(lr=0.1)\n",
    "cnet.compile(loss=\"categorical_crossentropy\", optimizer=copt, metrics=[\"accuracy\"])\n",
    "history = cnet.fit(train1[0], train1_y, validation_data=(valid1[0], valid1_y),\n",
    "\t            epochs=100, batch_size=16)\n",
    "\n",
    "#score = cnet.evaluate(test_data, test_labels)\n",
    "#print(\"Scores: %s\" % score)\n",
    "classify = predict_classes(cnet,test_data)\n",
    "print(theautil.classifications(classify,test_labels))\n",
    "score = cnet.evaluate(test_data, test1_y)\n",
    "print(\"Scores: %s\" % score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Experiment 2\n",
    "\n",
    "* Given 40 samples what distribution does it come from?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "########################################################################\n",
      "# Experiment 2: can we classify a sample of data?\n",
      "#\n",
      "#\n",
      "#########################################################################\n",
      "\n",
      "In this example we're going to input 40 values from a single distribution, and we'll see if we can classify the distribution.\n",
      "At this point we have a weird decision to make, how many neurons in the hidden layer?\n",
      "Epoch 1/100\n",
      " 1/23 [>.............................] - ETA: 3s - loss: 1.4025 - accuracy: 0.2500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hindle1/.local/lib/python3.8/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 6ms/step - loss: 1.3948 - accuracy: 0.2750 - val_loss: 1.3926 - val_accuracy: 0.1750\n",
      "Epoch 2/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1.3739 - accuracy: 0.2889 - val_loss: 1.3639 - val_accuracy: 0.4000\n",
      "Epoch 3/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1.3589 - accuracy: 0.3639 - val_loss: 1.3487 - val_accuracy: 0.3500\n",
      "Epoch 4/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1.3371 - accuracy: 0.4250 - val_loss: 1.3200 - val_accuracy: 0.4250\n",
      "Epoch 5/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1.3134 - accuracy: 0.4806 - val_loss: 1.2941 - val_accuracy: 0.4250\n",
      "Epoch 6/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1.2807 - accuracy: 0.4333 - val_loss: 1.2697 - val_accuracy: 0.3500\n",
      "Epoch 7/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1.2437 - accuracy: 0.4667 - val_loss: 1.2149 - val_accuracy: 0.3750\n",
      "Epoch 8/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.1960 - accuracy: 0.4667 - val_loss: 1.1578 - val_accuracy: 0.4750\n",
      "Epoch 9/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.1397 - accuracy: 0.4861 - val_loss: 1.0856 - val_accuracy: 0.5750\n",
      "Epoch 10/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.0781 - accuracy: 0.4861 - val_loss: 1.0418 - val_accuracy: 0.4500\n",
      "Epoch 11/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.0175 - accuracy: 0.5139 - val_loss: 0.9758 - val_accuracy: 0.5750\n",
      "Epoch 12/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.9628 - accuracy: 0.4806 - val_loss: 0.9352 - val_accuracy: 0.4500\n",
      "Epoch 13/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.9182 - accuracy: 0.4833 - val_loss: 0.9016 - val_accuracy: 0.4500\n",
      "Epoch 14/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.8813 - accuracy: 0.5139 - val_loss: 0.8551 - val_accuracy: 0.6500\n",
      "Epoch 15/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.8519 - accuracy: 0.4833 - val_loss: 0.8416 - val_accuracy: 0.4500\n",
      "Epoch 16/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.8270 - accuracy: 0.4722 - val_loss: 0.8207 - val_accuracy: 0.4500\n",
      "Epoch 17/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.8078 - accuracy: 0.5028 - val_loss: 0.7929 - val_accuracy: 0.7500\n",
      "Epoch 18/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7931 - accuracy: 0.5000 - val_loss: 0.7829 - val_accuracy: 0.3500\n",
      "Epoch 19/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.7785 - accuracy: 0.5139 - val_loss: 0.8000 - val_accuracy: 0.5250\n",
      "Epoch 20/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.7715 - accuracy: 0.5028 - val_loss: 0.7593 - val_accuracy: 0.6250\n",
      "Epoch 21/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.7636 - accuracy: 0.5056 - val_loss: 0.7661 - val_accuracy: 0.4500\n",
      "Epoch 22/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7574 - accuracy: 0.5056 - val_loss: 0.7476 - val_accuracy: 0.6750\n",
      "Epoch 23/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.7502 - accuracy: 0.4917 - val_loss: 0.7336 - val_accuracy: 0.5750\n",
      "Epoch 24/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.7451 - accuracy: 0.4806 - val_loss: 0.7349 - val_accuracy: 0.6250\n",
      "Epoch 25/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7417 - accuracy: 0.4861 - val_loss: 0.7400 - val_accuracy: 0.4500\n",
      "Epoch 26/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.7360 - accuracy: 0.5389 - val_loss: 0.7374 - val_accuracy: 0.4500\n",
      "Epoch 27/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.7336 - accuracy: 0.5083 - val_loss: 0.7330 - val_accuracy: 0.4750\n",
      "Epoch 28/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.7290 - accuracy: 0.5222 - val_loss: 0.7210 - val_accuracy: 0.6250\n",
      "Epoch 29/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.7277 - accuracy: 0.5222 - val_loss: 0.7203 - val_accuracy: 0.6000\n",
      "Epoch 30/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7244 - accuracy: 0.5000 - val_loss: 0.7084 - val_accuracy: 0.6250\n",
      "Epoch 31/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.7238 - accuracy: 0.5167 - val_loss: 0.7160 - val_accuracy: 0.5750\n",
      "Epoch 32/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7193 - accuracy: 0.5444 - val_loss: 0.7075 - val_accuracy: 0.6250\n",
      "Epoch 33/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7166 - accuracy: 0.5306 - val_loss: 0.7058 - val_accuracy: 0.6250\n",
      "Epoch 34/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.7160 - accuracy: 0.4972 - val_loss: 0.7124 - val_accuracy: 0.6250\n",
      "Epoch 35/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.7155 - accuracy: 0.5417 - val_loss: 0.7211 - val_accuracy: 0.4500\n",
      "Epoch 36/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.7108 - accuracy: 0.5417 - val_loss: 0.7262 - val_accuracy: 0.4500\n",
      "Epoch 37/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.7132 - accuracy: 0.5417 - val_loss: 0.7042 - val_accuracy: 0.6250\n",
      "Epoch 38/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7073 - accuracy: 0.5500 - val_loss: 0.6970 - val_accuracy: 0.6250\n",
      "Epoch 39/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7095 - accuracy: 0.5167 - val_loss: 0.7049 - val_accuracy: 0.6250\n",
      "Epoch 40/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.7066 - accuracy: 0.5361 - val_loss: 0.6929 - val_accuracy: 0.6250\n",
      "Epoch 41/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.7066 - accuracy: 0.5500 - val_loss: 0.6960 - val_accuracy: 0.6500\n",
      "Epoch 42/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.7045 - accuracy: 0.5806 - val_loss: 0.6987 - val_accuracy: 0.6250\n",
      "Epoch 43/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.7018 - accuracy: 0.5667 - val_loss: 0.6886 - val_accuracy: 0.6250\n",
      "Epoch 44/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.7015 - accuracy: 0.5722 - val_loss: 0.6911 - val_accuracy: 0.6000\n",
      "Epoch 45/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6999 - accuracy: 0.6056 - val_loss: 0.6992 - val_accuracy: 0.6750\n",
      "Epoch 46/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6948 - accuracy: 0.5778 - val_loss: 0.6894 - val_accuracy: 0.6000\n",
      "Epoch 47/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6989 - accuracy: 0.5944 - val_loss: 0.6885 - val_accuracy: 0.7000\n",
      "Epoch 48/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6972 - accuracy: 0.5861 - val_loss: 0.6916 - val_accuracy: 0.6500\n",
      "Epoch 49/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6936 - accuracy: 0.6111 - val_loss: 0.6924 - val_accuracy: 0.6500\n",
      "Epoch 50/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6928 - accuracy: 0.5861 - val_loss: 0.7055 - val_accuracy: 0.3250\n",
      "Epoch 51/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6912 - accuracy: 0.5833 - val_loss: 0.6941 - val_accuracy: 0.6500\n",
      "Epoch 52/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6880 - accuracy: 0.5556 - val_loss: 0.6796 - val_accuracy: 0.6000\n",
      "Epoch 53/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6892 - accuracy: 0.5944 - val_loss: 0.6807 - val_accuracy: 0.5500\n",
      "Epoch 54/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6889 - accuracy: 0.6167 - val_loss: 0.6962 - val_accuracy: 0.5000\n",
      "Epoch 55/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6868 - accuracy: 0.6167 - val_loss: 0.6990 - val_accuracy: 0.4750\n",
      "Epoch 56/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6876 - accuracy: 0.5833 - val_loss: 0.6905 - val_accuracy: 0.7250\n",
      "Epoch 57/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6848 - accuracy: 0.6306 - val_loss: 0.6905 - val_accuracy: 0.6750\n",
      "Epoch 58/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6833 - accuracy: 0.5528 - val_loss: 0.6780 - val_accuracy: 0.5250\n",
      "Epoch 59/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6824 - accuracy: 0.6250 - val_loss: 0.6929 - val_accuracy: 0.6000\n",
      "Epoch 60/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6825 - accuracy: 0.6167 - val_loss: 0.6847 - val_accuracy: 0.6000\n",
      "Epoch 61/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6804 - accuracy: 0.5972 - val_loss: 0.6832 - val_accuracy: 0.6000\n",
      "Epoch 62/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6796 - accuracy: 0.6111 - val_loss: 0.6841 - val_accuracy: 0.7250\n",
      "Epoch 63/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6758 - accuracy: 0.6056 - val_loss: 0.7007 - val_accuracy: 0.3750\n",
      "Epoch 64/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6767 - accuracy: 0.5806 - val_loss: 0.6771 - val_accuracy: 0.6750\n",
      "Epoch 65/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6763 - accuracy: 0.6056 - val_loss: 0.6749 - val_accuracy: 0.6500\n",
      "Epoch 66/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6760 - accuracy: 0.6056 - val_loss: 0.6890 - val_accuracy: 0.6000\n",
      "Epoch 67/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6721 - accuracy: 0.6111 - val_loss: 0.6721 - val_accuracy: 0.7000\n",
      "Epoch 68/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6720 - accuracy: 0.6417 - val_loss: 0.6852 - val_accuracy: 0.6000\n",
      "Epoch 69/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6705 - accuracy: 0.5917 - val_loss: 0.6739 - val_accuracy: 0.6750\n",
      "Epoch 70/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6679 - accuracy: 0.5833 - val_loss: 0.6631 - val_accuracy: 0.7000\n",
      "Epoch 71/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6697 - accuracy: 0.6306 - val_loss: 0.6701 - val_accuracy: 0.5250\n",
      "Epoch 72/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6699 - accuracy: 0.6167 - val_loss: 0.6696 - val_accuracy: 0.6500\n",
      "Epoch 73/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6673 - accuracy: 0.6028 - val_loss: 0.6650 - val_accuracy: 0.6250\n",
      "Epoch 74/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6651 - accuracy: 0.6111 - val_loss: 0.6766 - val_accuracy: 0.6250\n",
      "Epoch 75/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6575 - accuracy: 0.5972 - val_loss: 0.6574 - val_accuracy: 0.6000\n",
      "Epoch 76/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6647 - accuracy: 0.6333 - val_loss: 0.6681 - val_accuracy: 0.5750\n",
      "Epoch 77/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6628 - accuracy: 0.6444 - val_loss: 0.6860 - val_accuracy: 0.5500\n",
      "Epoch 78/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6590 - accuracy: 0.6222 - val_loss: 0.6642 - val_accuracy: 0.6500\n",
      "Epoch 79/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6594 - accuracy: 0.6222 - val_loss: 0.6616 - val_accuracy: 0.5750\n",
      "Epoch 80/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6557 - accuracy: 0.6278 - val_loss: 0.6889 - val_accuracy: 0.6250\n",
      "Epoch 81/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6577 - accuracy: 0.5889 - val_loss: 0.6619 - val_accuracy: 0.6500\n",
      "Epoch 82/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6566 - accuracy: 0.6167 - val_loss: 0.6774 - val_accuracy: 0.6750\n",
      "Epoch 83/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6563 - accuracy: 0.6111 - val_loss: 0.6704 - val_accuracy: 0.6750\n",
      "Epoch 84/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6518 - accuracy: 0.6722 - val_loss: 0.6743 - val_accuracy: 0.6000\n",
      "Epoch 85/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6528 - accuracy: 0.6222 - val_loss: 0.6855 - val_accuracy: 0.6000\n",
      "Epoch 86/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6533 - accuracy: 0.6278 - val_loss: 0.6890 - val_accuracy: 0.4750\n",
      "Epoch 87/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6489 - accuracy: 0.6194 - val_loss: 0.6590 - val_accuracy: 0.5750\n",
      "Epoch 88/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6522 - accuracy: 0.6278 - val_loss: 0.6534 - val_accuracy: 0.5500\n",
      "Epoch 89/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6481 - accuracy: 0.6500 - val_loss: 0.6678 - val_accuracy: 0.7000\n",
      "Epoch 90/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6465 - accuracy: 0.6556 - val_loss: 0.6913 - val_accuracy: 0.4500\n",
      "Epoch 91/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6472 - accuracy: 0.6139 - val_loss: 0.6750 - val_accuracy: 0.6250\n",
      "Epoch 92/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6465 - accuracy: 0.6194 - val_loss: 0.6603 - val_accuracy: 0.7000\n",
      "Epoch 93/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6424 - accuracy: 0.6528 - val_loss: 0.6742 - val_accuracy: 0.6250\n",
      "Epoch 94/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6441 - accuracy: 0.6389 - val_loss: 0.6667 - val_accuracy: 0.6500\n",
      "Epoch 95/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6406 - accuracy: 0.6556 - val_loss: 0.6628 - val_accuracy: 0.6500\n",
      "Epoch 96/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6419 - accuracy: 0.6056 - val_loss: 0.6681 - val_accuracy: 0.7500\n",
      "Epoch 97/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6380 - accuracy: 0.6333 - val_loss: 0.6518 - val_accuracy: 0.6000\n",
      "Epoch 98/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6388 - accuracy: 0.6556 - val_loss: 0.6534 - val_accuracy: 0.5250\n",
      "Epoch 99/100\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6381 - accuracy: 0.6528 - val_loss: 0.6606 - val_accuracy: 0.6250\n",
      "Epoch 100/100\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6385 - accuracy: 0.6444 - val_loss: 0.6640 - val_accuracy: 0.6250\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.6158 - accuracy: 0.6725\n",
      "['loss', 'accuracy']\n",
      "Scores: [0.6158366799354553, 0.6725000143051147]\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "########################################################################\n",
    "# Experiment 2: can we classify a sample of data?\n",
    "#\n",
    "#\n",
    "#########################################################################\n",
    "''')\n",
    "print(\"In this example we're going to input 40 values from a single distribution, and we'll see if we can classify the distribution.\")\n",
    "\n",
    "width=40\n",
    "\n",
    "def make_widedataset(width=width):\n",
    "    # we're going to make rows of 40 features unsorted\n",
    "    wlns      = min_max_scale(lognormal(size=(bsize,width))) #log normal\n",
    "    wpowers   = min_max_scale(power(0.1,size=(bsize,width))) #power law\n",
    "    wnorms    = min_max_scale(normal(size=(bsize,width)))    #normal\n",
    "    wuniforms = min_max_scale(uniform(size=(bsize,width)))    #uniform\n",
    "    \n",
    "    wdata = np.concatenate((wlns,wpowers,wnorms,wuniforms))\n",
    "    \n",
    "    # concatenate our labels\n",
    "    wlabels = np.concatenate((\n",
    "        (np.repeat(LOGNORMAL,bsize)),\n",
    "        (np.repeat(POWER,bsize)),\n",
    "        (np.repeat(NORM,bsize)),\n",
    "        (np.repeat(UNIFORM,bsize))))\n",
    "    \n",
    "    joint_shuffle(wdata,wlabels)\n",
    "    wdata = wdata.astype(np.float32)\n",
    "    wlabels = wlabels.astype(np.int32)\n",
    "    wlabels = wlabels.reshape((len(data),))\n",
    "    return wdata, wlabels\n",
    "\n",
    "# make our train sets\n",
    "wdata, wlabels = make_widedataset()\n",
    "# make our test sets\n",
    "test_wdata, test_wlabels = make_widedataset()\n",
    "\n",
    "# split out our validation set\n",
    "wtrain, wvalid = split_validation(90, wdata, wlabels)\n",
    "print(\"At this point we have a weird decision to make, how many neurons in the hidden layer?\")\n",
    "\n",
    "encwc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "encwc.fit(wtrain[1].reshape(len(wtrain[1]),1))\n",
    "wtrain_y = encwc.transform(wtrain[1].reshape(len(wtrain[1]),1))\n",
    "wvalid_y = encwc.transform(wvalid[1].reshape(len(wvalid[1]),1))\n",
    "wtest_y  = encwc.transform(test_wlabels.reshape(len(test_wlabels),1))\n",
    "\n",
    "# wcnet = theanets.Classifier([width,width/4,4]) #267\n",
    "wcnet = Sequential()\n",
    "wcnet.add(Dense(width,input_shape=(width,),activation=\"sigmoid\"))\n",
    "wcnet.add(Dense(int(width/4),activation=\"sigmoid\"))\n",
    "wcnet.add(Dense(4,activation=\"softmax\"))\n",
    "wcnet.compile(loss=\"categorical_crossentropy\", optimizer=SGD(lr=0.1), metrics=[\"accuracy\"])\n",
    "history = wcnet.fit(wtrain[0], wtrain_y, validation_data=(wvalid[0], wvalid_y),\n",
    "\t            epochs=100, batch_size=16)\n",
    "score = wcnet.evaluate(test_wdata, wtest_y)\n",
    "print(wcnet.metrics_names)\n",
    "print(\"Scores: %s\" % score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tp', 95), ('tn', 73), ('fp', 27), ('fn', 5)]\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5327 - accuracy: 0.6700\n",
      "Scores: [0.5327289700508118, 0.6700000166893005]\n",
      "Ok that was neat, it definitely worked better, it had more data though.\n",
      "But what if we help it out, and we sort the values so that the first and last bins are always the min and max values?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "classify = wcnet.predict_classes(test_wdata)\n",
    "print(theautil.classifications(classify,test_wlabels))\n",
    "score = wcnet.evaluate(test_wdata, wtest_y)\n",
    "print(\"Scores: %s\" % score)\n",
    "\n",
    "# # You could try some of these alternative setups\n",
    "# \n",
    "# [width,4]) #248\n",
    "# [width,width/2,4]) #271\n",
    "# [width,width,4]) #289\n",
    "# [width,width*2,4]) #292\n",
    "# [width,width/2,width/4,4]) #270\n",
    "# [width,width/2,width/4,width/8,width/16,4]) #232\n",
    "# [width,width*8,4]) #304\n",
    "\n",
    "print(\"Ok that was neat, it definitely worked better, it had more data though.\")\n",
    "\n",
    "print(\"But what if we help it out, and we sort the values so that the first and last bins are always the min and max values?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Experiment 3\n",
    "\n",
    "* Given 40 sorted samples what distribution does it come from?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "########################################################################\n",
      "# Experiment 3: can we classify a SORTED sample of data?\n",
      "#\n",
      "#\n",
      "#########################################################################\n",
      "\n",
      "Sorting the data\n",
      "Epoch 1/100\n",
      "23/23 [==============================] - 1s 25ms/step - loss: 1.3747 - accuracy: 0.3611 - val_loss: 1.3424 - val_accuracy: 0.3500\n",
      "Epoch 2/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.3360 - accuracy: 0.4000 - val_loss: 1.3254 - val_accuracy: 0.5250\n",
      "Epoch 3/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.3151 - accuracy: 0.4389 - val_loss: 1.3122 - val_accuracy: 0.4000\n",
      "Epoch 4/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.2784 - accuracy: 0.4833 - val_loss: 1.2773 - val_accuracy: 0.2000\n",
      "Epoch 5/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.2318 - accuracy: 0.5417 - val_loss: 1.2267 - val_accuracy: 0.4750\n",
      "Epoch 6/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.1827 - accuracy: 0.5306 - val_loss: 1.1604 - val_accuracy: 0.4750\n",
      "Epoch 7/100\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.1243 - accuracy: 0.5556 - val_loss: 1.0821 - val_accuracy: 0.7250\n",
      "Epoch 8/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.0594 - accuracy: 0.5833 - val_loss: 1.0228 - val_accuracy: 0.4750\n",
      "Epoch 9/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.9923 - accuracy: 0.5611 - val_loss: 0.9647 - val_accuracy: 0.6500\n",
      "Epoch 10/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.9391 - accuracy: 0.6444 - val_loss: 0.9013 - val_accuracy: 0.7250\n",
      "Epoch 11/100\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.8853 - accuracy: 0.6583 - val_loss: 0.8520 - val_accuracy: 0.7750\n",
      "Epoch 12/100\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.8455 - accuracy: 0.7056 - val_loss: 0.8178 - val_accuracy: 0.6250\n",
      "Epoch 13/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.8102 - accuracy: 0.7000 - val_loss: 0.7909 - val_accuracy: 0.4750\n",
      "Epoch 14/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.7792 - accuracy: 0.6389 - val_loss: 0.7615 - val_accuracy: 0.8000\n",
      "Epoch 15/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.7625 - accuracy: 0.7417 - val_loss: 0.7361 - val_accuracy: 0.9000\n",
      "Epoch 16/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.7376 - accuracy: 0.7667 - val_loss: 0.7187 - val_accuracy: 0.7500\n",
      "Epoch 17/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.7226 - accuracy: 0.7361 - val_loss: 0.7052 - val_accuracy: 0.7750\n",
      "Epoch 18/100\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.7068 - accuracy: 0.7306 - val_loss: 0.6796 - val_accuracy: 0.8500\n",
      "Epoch 19/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.6909 - accuracy: 0.7583 - val_loss: 0.6737 - val_accuracy: 0.5750\n",
      "Epoch 20/100\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6772 - accuracy: 0.6917 - val_loss: 0.6505 - val_accuracy: 0.8750\n",
      "Epoch 21/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.6681 - accuracy: 0.7861 - val_loss: 0.6377 - val_accuracy: 0.7750\n",
      "Epoch 22/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.6556 - accuracy: 0.7278 - val_loss: 0.6205 - val_accuracy: 0.8750\n",
      "Epoch 23/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.6426 - accuracy: 0.7750 - val_loss: 0.6080 - val_accuracy: 0.7750\n",
      "Epoch 24/100\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.6321 - accuracy: 0.7722 - val_loss: 0.5967 - val_accuracy: 0.8000\n",
      "Epoch 25/100\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.6206 - accuracy: 0.7556 - val_loss: 0.5763 - val_accuracy: 0.8750\n",
      "Epoch 26/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.6065 - accuracy: 0.7944 - val_loss: 0.5614 - val_accuracy: 0.8000\n",
      "Epoch 27/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.5930 - accuracy: 0.8056 - val_loss: 0.5441 - val_accuracy: 0.9250\n",
      "Epoch 28/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.5799 - accuracy: 0.8444 - val_loss: 0.5293 - val_accuracy: 0.9500\n",
      "Epoch 29/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.5680 - accuracy: 0.8778 - val_loss: 0.5132 - val_accuracy: 0.9500\n",
      "Epoch 30/100\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.5552 - accuracy: 0.8278 - val_loss: 0.5016 - val_accuracy: 0.8500\n",
      "Epoch 31/100\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.5401 - accuracy: 0.7861 - val_loss: 0.4791 - val_accuracy: 0.9500\n",
      "Epoch 32/100\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.5282 - accuracy: 0.8361 - val_loss: 0.4626 - val_accuracy: 0.8250\n",
      "Epoch 33/100\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.5164 - accuracy: 0.8472 - val_loss: 0.4519 - val_accuracy: 0.9250\n",
      "Epoch 34/100\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.5018 - accuracy: 0.9417 - val_loss: 0.4405 - val_accuracy: 0.9500\n",
      "Epoch 35/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.4898 - accuracy: 0.9000 - val_loss: 0.4200 - val_accuracy: 0.9250\n",
      "Epoch 36/100\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.4727 - accuracy: 0.9472 - val_loss: 0.4088 - val_accuracy: 0.9500\n",
      "Epoch 37/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.4597 - accuracy: 0.9500 - val_loss: 0.3890 - val_accuracy: 0.8250\n",
      "Epoch 38/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.4441 - accuracy: 0.8972 - val_loss: 0.3727 - val_accuracy: 0.9750\n",
      "Epoch 39/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.4288 - accuracy: 0.9694 - val_loss: 0.3528 - val_accuracy: 0.9750\n",
      "Epoch 40/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.4132 - accuracy: 0.9667 - val_loss: 0.3389 - val_accuracy: 0.9750\n",
      "Epoch 41/100\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.3977 - accuracy: 0.9722 - val_loss: 0.3203 - val_accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.3765 - accuracy: 0.9667 - val_loss: 0.3027 - val_accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.3619 - accuracy: 0.9806 - val_loss: 0.2883 - val_accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.3434 - accuracy: 0.9861 - val_loss: 0.2708 - val_accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.3256 - accuracy: 0.9833 - val_loss: 0.2535 - val_accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.3062 - accuracy: 0.9833 - val_loss: 0.2436 - val_accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2904 - accuracy: 0.9861 - val_loss: 0.2356 - val_accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2734 - accuracy: 0.9861 - val_loss: 0.2099 - val_accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2556 - accuracy: 0.9833 - val_loss: 0.2025 - val_accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2391 - accuracy: 0.9778 - val_loss: 0.1826 - val_accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2262 - accuracy: 0.9833 - val_loss: 0.1740 - val_accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.2109 - accuracy: 0.9889 - val_loss: 0.1669 - val_accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.1975 - accuracy: 0.9861 - val_loss: 0.1480 - val_accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.1863 - accuracy: 0.9889 - val_loss: 0.1449 - val_accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.1747 - accuracy: 0.9917 - val_loss: 0.1333 - val_accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.1648 - accuracy: 0.9917 - val_loss: 0.1245 - val_accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.1547 - accuracy: 0.9917 - val_loss: 0.1132 - val_accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.1475 - accuracy: 0.9917 - val_loss: 0.1075 - val_accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.1384 - accuracy: 0.9917 - val_loss: 0.1030 - val_accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.1317 - accuracy: 0.9917 - val_loss: 0.0991 - val_accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.1254 - accuracy: 0.9917 - val_loss: 0.0903 - val_accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.1203 - accuracy: 0.9917 - val_loss: 0.0864 - val_accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.1140 - accuracy: 0.9917 - val_loss: 0.0840 - val_accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.1091 - accuracy: 0.9917 - val_loss: 0.0799 - val_accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.1047 - accuracy: 0.9917 - val_loss: 0.0764 - val_accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.1010 - accuracy: 0.9917 - val_loss: 0.0726 - val_accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.0971 - accuracy: 0.9917 - val_loss: 0.0721 - val_accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0929 - accuracy: 0.9917 - val_loss: 0.0706 - val_accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0900 - accuracy: 0.9917 - val_loss: 0.0650 - val_accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0869 - accuracy: 0.9917 - val_loss: 0.0608 - val_accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0843 - accuracy: 0.9917 - val_loss: 0.0616 - val_accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0808 - accuracy: 0.9917 - val_loss: 0.0588 - val_accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.0788 - accuracy: 0.9917 - val_loss: 0.0570 - val_accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0765 - accuracy: 0.9917 - val_loss: 0.0552 - val_accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0749 - accuracy: 0.9917 - val_loss: 0.0525 - val_accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0722 - accuracy: 0.9917 - val_loss: 0.0502 - val_accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0710 - accuracy: 0.9917 - val_loss: 0.0519 - val_accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0693 - accuracy: 0.9917 - val_loss: 0.0482 - val_accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0675 - accuracy: 0.9917 - val_loss: 0.0460 - val_accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0657 - accuracy: 0.9917 - val_loss: 0.0456 - val_accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0649 - accuracy: 0.9917 - val_loss: 0.0443 - val_accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.0633 - accuracy: 0.9917 - val_loss: 0.0429 - val_accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0613 - accuracy: 0.9917 - val_loss: 0.0423 - val_accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.0605 - accuracy: 0.9917 - val_loss: 0.0390 - val_accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.0590 - accuracy: 0.9917 - val_loss: 0.0403 - val_accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.0584 - accuracy: 0.9917 - val_loss: 0.0380 - val_accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0567 - accuracy: 0.9917 - val_loss: 0.0399 - val_accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.0558 - accuracy: 0.9917 - val_loss: 0.0387 - val_accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0544 - accuracy: 0.9917 - val_loss: 0.0366 - val_accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0538 - accuracy: 0.9917 - val_loss: 0.0347 - val_accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0530 - accuracy: 0.9917 - val_loss: 0.0345 - val_accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0530 - accuracy: 0.9917 - val_loss: 0.0338 - val_accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0514 - accuracy: 0.9917 - val_loss: 0.0329 - val_accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.0500 - accuracy: 0.9917 - val_loss: 0.0341 - val_accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0500 - accuracy: 0.9917 - val_loss: 0.0320 - val_accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0487 - accuracy: 0.9917 - val_loss: 0.0333 - val_accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0483 - accuracy: 0.9917 - val_loss: 0.0324 - val_accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0475 - accuracy: 0.9917 - val_loss: 0.0305 - val_accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.0467 - accuracy: 0.9917 - val_loss: 0.0307 - val_accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0467 - accuracy: 0.99 - 0s 15ms/step - loss: 0.0463 - accuracy: 0.9917 - val_loss: 0.0293 - val_accuracy: 1.0000\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0717 - accuracy: 0.9825\n",
      "Scores: [0.07169201970100403, 0.9825000166893005]\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "########################################################################\n",
    "# Experiment 3: can we classify a SORTED sample of data?\n",
    "#\n",
    "#\n",
    "#########################################################################\n",
    "''')\n",
    "\n",
    "\n",
    "print(\"Sorting the data\")\n",
    "wdata.sort(axis=1)\n",
    "test_wdata.sort(axis=1)\n",
    "\n",
    "\n",
    "swcnet = Sequential()\n",
    "swcnet.add(Dense(width,input_shape=(width,),activation=\"sigmoid\"))\n",
    "swcnet.add(Dense(int(width/4),activation=\"sigmoid\"))\n",
    "swcnet.add(Dense(4,activation=\"softmax\"))\n",
    "swcnet.compile(loss=\"categorical_crossentropy\", optimizer=SGD(lr=0.1), metrics=[\"accuracy\"])\n",
    "history = swcnet.fit(wtrain[0], wtrain_y, validation_data=(wvalid[0], wvalid_y),\n",
    "\t            epochs=100, batch_size=16)\n",
    "score = swcnet.evaluate(test_wdata, wtest_y)\n",
    "print(\"Scores: %s\" % score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tp', 98), ('tn', 96), ('fp', 4), ('fn', 2)]\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.0717 - accuracy: 0.9825\n",
      "Scores: [0.07169201970100403, 0.9825000166893005]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "classify = swcnet.predict_classes(test_wdata)\n",
    "print(theautil.classifications(classify,test_wlabels))\n",
    "score = swcnet.evaluate(test_wdata, wtest_y)\n",
    "print(\"Scores: %s\" % score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was an improvement. What if we do binning instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0458 - accuracy: 0.9917 - val_loss: 0.0285 - val_accuracy: 1.0000\n",
      "Epoch 2/100\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0449 - accuracy: 0.9917 - val_loss: 0.0288 - val_accuracy: 1.0000\n",
      "Epoch 3/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0440 - accuracy: 0.9917 - val_loss: 0.0265 - val_accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.0437 - accuracy: 0.9917 - val_loss: 0.0262 - val_accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.0437 - accuracy: 0.9917 - val_loss: 0.0270 - val_accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.0430 - accuracy: 0.9917 - val_loss: 0.0263 - val_accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0419 - accuracy: 0.9917 - val_loss: 0.0258 - val_accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0419 - accuracy: 0.9917 - val_loss: 0.0257 - val_accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0407 - accuracy: 0.9917 - val_loss: 0.0265 - val_accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.0411 - accuracy: 0.9917 - val_loss: 0.0246 - val_accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.0402 - accuracy: 0.9917 - val_loss: 0.0260 - val_accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0399 - accuracy: 0.9917 - val_loss: 0.0232 - val_accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0391 - accuracy: 0.9917 - val_loss: 0.0246 - val_accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0391 - accuracy: 0.9917 - val_loss: 0.0233 - val_accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0385 - accuracy: 0.9917 - val_loss: 0.0223 - val_accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0383 - accuracy: 0.9917 - val_loss: 0.0237 - val_accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.0383 - accuracy: 0.9917 - val_loss: 0.0223 - val_accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0375 - accuracy: 0.9917 - val_loss: 0.0235 - val_accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0376 - accuracy: 0.9917 - val_loss: 0.0218 - val_accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.0366 - accuracy: 0.9917 - val_loss: 0.0220 - val_accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0362 - accuracy: 0.9917 - val_loss: 0.0208 - val_accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0364 - accuracy: 0.9917 - val_loss: 0.0207 - val_accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0361 - accuracy: 0.9917 - val_loss: 0.0210 - val_accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.0354 - accuracy: 0.9917 - val_loss: 0.0200 - val_accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.0355 - accuracy: 0.9917 - val_loss: 0.0209 - val_accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.0350 - accuracy: 0.9917 - val_loss: 0.0205 - val_accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0352 - accuracy: 0.9917 - val_loss: 0.0192 - val_accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.0347 - accuracy: 0.9917 - val_loss: 0.0196 - val_accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0341 - accuracy: 0.9917 - val_loss: 0.0193 - val_accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.0338 - accuracy: 0.9917 - val_loss: 0.0199 - val_accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.0337 - accuracy: 0.9917 - val_loss: 0.0193 - val_accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0340 - accuracy: 0.9917 - val_loss: 0.0204 - val_accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0336 - accuracy: 0.9917 - val_loss: 0.0213 - val_accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0333 - accuracy: 0.9917 - val_loss: 0.0210 - val_accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0331 - accuracy: 0.9917 - val_loss: 0.0190 - val_accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0324 - accuracy: 0.9917 - val_loss: 0.0186 - val_accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0325 - accuracy: 0.9917 - val_loss: 0.0204 - val_accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0319 - accuracy: 0.9917 - val_loss: 0.0168 - val_accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0319 - accuracy: 0.9917 - val_loss: 0.0174 - val_accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0317 - accuracy: 0.9917 - val_loss: 0.0173 - val_accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.0314 - accuracy: 0.9917 - val_loss: 0.0176 - val_accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.0313 - accuracy: 0.9917 - val_loss: 0.0166 - val_accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.0311 - accuracy: 0.9917 - val_loss: 0.0169 - val_accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.0307 - accuracy: 0.9917 - val_loss: 0.0174 - val_accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.0308 - accuracy: 0.9917 - val_loss: 0.0164 - val_accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0302 - accuracy: 0.9917 - val_loss: 0.0169 - val_accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.0304 - accuracy: 0.9917 - val_loss: 0.0168 - val_accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0298 - accuracy: 0.9917 - val_loss: 0.0154 - val_accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0301 - accuracy: 0.9917 - val_loss: 0.0156 - val_accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0296 - accuracy: 0.9917 - val_loss: 0.0163 - val_accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.0296 - accuracy: 0.9917 - val_loss: 0.0152 - val_accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0293 - accuracy: 0.9917 - val_loss: 0.0151 - val_accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.0292 - accuracy: 0.9917 - val_loss: 0.0157 - val_accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.0288 - accuracy: 0.9917 - val_loss: 0.0176 - val_accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0290 - accuracy: 0.9917 - val_loss: 0.0167 - val_accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.0289 - accuracy: 0.9917 - val_loss: 0.0158 - val_accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0286 - accuracy: 0.9917 - val_loss: 0.0153 - val_accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0280 - accuracy: 0.9917 - val_loss: 0.0141 - val_accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.0282 - accuracy: 0.9917 - val_loss: 0.0154 - val_accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0281 - accuracy: 0.9917 - val_loss: 0.0147 - val_accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0278 - accuracy: 0.9917 - val_loss: 0.0143 - val_accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0272 - accuracy: 0.9917 - val_loss: 0.0155 - val_accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0280 - accuracy: 0.9917 - val_loss: 0.0151 - val_accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0275 - accuracy: 0.9917 - val_loss: 0.0145 - val_accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.0273 - accuracy: 0.9917 - val_loss: 0.0144 - val_accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.0272 - accuracy: 0.9917 - val_loss: 0.0134 - val_accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0270 - accuracy: 0.9917 - val_loss: 0.0130 - val_accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0270 - accuracy: 0.9917 - val_loss: 0.0137 - val_accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0266 - accuracy: 0.9917 - val_loss: 0.0141 - val_accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0266 - accuracy: 0.9917 - val_loss: 0.0132 - val_accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0272 - accuracy: 0.9917 - val_loss: 0.0128 - val_accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0262 - accuracy: 0.9917 - val_loss: 0.0134 - val_accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0263 - accuracy: 0.9917 - val_loss: 0.0129 - val_accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0258 - accuracy: 0.9917 - val_loss: 0.0120 - val_accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.0260 - accuracy: 0.9917 - val_loss: 0.0124 - val_accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.0257 - accuracy: 0.9917 - val_loss: 0.0136 - val_accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.0258 - accuracy: 0.9917 - val_loss: 0.0124 - val_accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0256 - accuracy: 0.9917 - val_loss: 0.0132 - val_accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.0251 - accuracy: 0.9917 - val_loss: 0.0120 - val_accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0256 - accuracy: 0.9917 - val_loss: 0.0127 - val_accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0251 - accuracy: 0.9917 - val_loss: 0.0130 - val_accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0249 - accuracy: 0.9917 - val_loss: 0.0116 - val_accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0250 - accuracy: 0.9917 - val_loss: 0.0126 - val_accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0252 - accuracy: 0.9917 - val_loss: 0.0123 - val_accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0243 - accuracy: 0.9917 - val_loss: 0.0120 - val_accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0245 - accuracy: 0.9917 - val_loss: 0.0115 - val_accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.0246 - accuracy: 0.9917 - val_loss: 0.0118 - val_accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0242 - accuracy: 0.9917 - val_loss: 0.0124 - val_accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0242 - accuracy: 0.9917 - val_loss: 0.0147 - val_accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0242 - accuracy: 0.9917 - val_loss: 0.0133 - val_accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0243 - accuracy: 0.9917 - val_loss: 0.0129 - val_accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0241 - accuracy: 0.9917 - val_loss: 0.0118 - val_accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.0239 - accuracy: 0.9917 - val_loss: 0.0122 - val_accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.0237 - accuracy: 0.9917 - val_loss: 0.0130 - val_accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.0240 - accuracy: 0.9917 - val_loss: 0.0122 - val_accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.0237 - accuracy: 0.9917 - val_loss: 0.0117 - val_accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.0235 - accuracy: 0.9917 - val_loss: 0.0115 - val_accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0236 - accuracy: 0.9917 - val_loss: 0.0117 - val_accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0228 - accuracy: 0.9917 - val_loss: 0.0108 - val_accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0232 - accuracy: 0.9917 - val_loss: 0.0117 - val_accuracy: 1.0000\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0596 - accuracy: 0.9775\n",
      "Scores: [0.059604860842227936, 0.9775000214576721]\n"
     ]
    }
   ],
   "source": [
    "history = swcnet.fit(wtrain[0], wtrain_y, validation_data=(wvalid[0], wvalid_y),\n",
    "\t            epochs=100, batch_size=16)\n",
    "score = swcnet.evaluate(test_wdata, wtest_y)\n",
    "print(\"Scores: %s\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tp', 98), ('tn', 94), ('fp', 6), ('fn', 2)]\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.0596 - accuracy: 0.9775\n",
      "Scores: [0.059604860842227936, 0.9775000214576721]\n"
     ]
    }
   ],
   "source": [
    "classify = swcnet.predict_classes(test_wdata)\n",
    "print(theautil.classifications(classify,test_wlabels))\n",
    "score = swcnet.evaluate(test_wdata, wtest_y)\n",
    "print(\"Scores: %s\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4\n",
    "\n",
    "* Given 40 histogrammed samples what distribution does it come from?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "########################################################################\n",
      "# Experiment 4: can we classify a discretized histogram of sample data?\n",
      "#\n",
      "#\n",
      "#########################################################################\n",
      "\n",
      "Apply the histogram to all the data rows\n",
      "Epoch 1/100\n",
      "23/23 [==============================] - 1s 28ms/step - loss: 1.4808 - accuracy: 0.2639 - val_loss: 1.3730 - val_accuracy: 0.3250\n",
      "Epoch 2/100\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.3908 - accuracy: 0.2417 - val_loss: 1.3748 - val_accuracy: 0.3250\n",
      "Epoch 3/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.3915 - accuracy: 0.2417 - val_loss: 1.3621 - val_accuracy: 0.3250\n",
      "Epoch 4/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3898 - accuracy: 0.2472 - val_loss: 1.3752 - val_accuracy: 0.3250\n",
      "Epoch 5/100\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.3897 - accuracy: 0.2389 - val_loss: 1.3940 - val_accuracy: 0.1500\n",
      "Epoch 6/100\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 1.3869 - accuracy: 0.2750 - val_loss: 1.4035 - val_accuracy: 0.1500\n",
      "Epoch 7/100\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3869 - accuracy: 0.2444 - val_loss: 1.3943 - val_accuracy: 0.1500\n",
      "Epoch 8/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.3871 - accuracy: 0.2500 - val_loss: 1.3932 - val_accuracy: 0.2000\n",
      "Epoch 9/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3835 - accuracy: 0.2306 - val_loss: 1.4143 - val_accuracy: 0.1500\n",
      "Epoch 10/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.3821 - accuracy: 0.2583 - val_loss: 1.3941 - val_accuracy: 0.1500\n",
      "Epoch 11/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.3809 - accuracy: 0.2583 - val_loss: 1.3875 - val_accuracy: 0.1500\n",
      "Epoch 12/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 1.3771 - accuracy: 0.2667 - val_loss: 1.3840 - val_accuracy: 0.3250\n",
      "Epoch 13/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 1.3791 - accuracy: 0.2722 - val_loss: 1.3540 - val_accuracy: 0.3250\n",
      "Epoch 14/100\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 1.3762 - accuracy: 0.2639 - val_loss: 1.3607 - val_accuracy: 0.5250\n",
      "Epoch 15/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.3720 - accuracy: 0.3528 - val_loss: 1.3832 - val_accuracy: 0.1500\n",
      "Epoch 16/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.3702 - accuracy: 0.3000 - val_loss: 1.3658 - val_accuracy: 0.3750\n",
      "Epoch 17/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.3677 - accuracy: 0.2556 - val_loss: 1.3632 - val_accuracy: 0.3000\n",
      "Epoch 18/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3603 - accuracy: 0.3194 - val_loss: 1.3375 - val_accuracy: 0.3250\n",
      "Epoch 19/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 1.3588 - accuracy: 0.3667 - val_loss: 1.3623 - val_accuracy: 0.4750\n",
      "Epoch 20/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.3515 - accuracy: 0.4000 - val_loss: 1.3538 - val_accuracy: 0.2750\n",
      "Epoch 21/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 1.3446 - accuracy: 0.3528 - val_loss: 1.3323 - val_accuracy: 0.3500\n",
      "Epoch 22/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 1.3450 - accuracy: 0.4139 - val_loss: 1.3660 - val_accuracy: 0.1500\n",
      "Epoch 23/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.3349 - accuracy: 0.3306 - val_loss: 1.3276 - val_accuracy: 0.7250\n",
      "Epoch 24/100\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.3231 - accuracy: 0.4000 - val_loss: 1.3039 - val_accuracy: 0.5250\n",
      "Epoch 25/100\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.3148 - accuracy: 0.4778 - val_loss: 1.3046 - val_accuracy: 0.5250\n",
      "Epoch 26/100\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 1.3022 - accuracy: 0.4778 - val_loss: 1.2946 - val_accuracy: 0.4750\n",
      "Epoch 27/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.2919 - accuracy: 0.4833 - val_loss: 1.2883 - val_accuracy: 0.4750\n",
      "Epoch 28/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.2749 - accuracy: 0.5222 - val_loss: 1.2736 - val_accuracy: 0.5250\n",
      "Epoch 29/100\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.2557 - accuracy: 0.5056 - val_loss: 1.2560 - val_accuracy: 0.5750\n",
      "Epoch 30/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.2377 - accuracy: 0.4833 - val_loss: 1.2432 - val_accuracy: 0.6000\n",
      "Epoch 31/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 1.2135 - accuracy: 0.4972 - val_loss: 1.2024 - val_accuracy: 0.7500\n",
      "Epoch 32/100\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 1.1917 - accuracy: 0.4889 - val_loss: 1.1860 - val_accuracy: 0.5500\n",
      "Epoch 33/100\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.1578 - accuracy: 0.5083 - val_loss: 1.1356 - val_accuracy: 0.5500\n",
      "Epoch 34/100\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.1284 - accuracy: 0.5083 - val_loss: 1.0811 - val_accuracy: 0.5250\n",
      "Epoch 35/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 1.0930 - accuracy: 0.5083 - val_loss: 1.1091 - val_accuracy: 0.7500\n",
      "Epoch 36/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 1.0595 - accuracy: 0.4972 - val_loss: 1.0275 - val_accuracy: 0.5250\n",
      "Epoch 37/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 1.0229 - accuracy: 0.5167 - val_loss: 0.9911 - val_accuracy: 0.5750\n",
      "Epoch 38/100\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.9887 - accuracy: 0.5444 - val_loss: 0.9577 - val_accuracy: 0.5250\n",
      "Epoch 39/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.9592 - accuracy: 0.5000 - val_loss: 0.9285 - val_accuracy: 0.5250\n",
      "Epoch 40/100\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.9243 - accuracy: 0.4917 - val_loss: 0.8926 - val_accuracy: 0.5250\n",
      "Epoch 41/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.8937 - accuracy: 0.5278 - val_loss: 0.8774 - val_accuracy: 0.6750\n",
      "Epoch 42/100\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.8712 - accuracy: 0.5556 - val_loss: 0.8452 - val_accuracy: 0.4750\n",
      "Epoch 43/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.8471 - accuracy: 0.5417 - val_loss: 0.8296 - val_accuracy: 0.5250\n",
      "Epoch 44/100\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.8334 - accuracy: 0.5250 - val_loss: 0.8102 - val_accuracy: 0.5250\n",
      "Epoch 45/100\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.8127 - accuracy: 0.5111 - val_loss: 0.7960 - val_accuracy: 0.6500\n",
      "Epoch 46/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.8000 - accuracy: 0.5722 - val_loss: 0.7775 - val_accuracy: 0.6000\n",
      "Epoch 47/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.7851 - accuracy: 0.5500 - val_loss: 0.7704 - val_accuracy: 0.7750\n",
      "Epoch 48/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.7809 - accuracy: 0.5111 - val_loss: 0.7576 - val_accuracy: 0.6500\n",
      "Epoch 49/100\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.7674 - accuracy: 0.5111 - val_loss: 0.7516 - val_accuracy: 0.5500\n",
      "Epoch 50/100\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.7608 - accuracy: 0.5361 - val_loss: 0.7482 - val_accuracy: 0.4750\n",
      "Epoch 51/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.7528 - accuracy: 0.5472 - val_loss: 0.7437 - val_accuracy: 0.6000\n",
      "Epoch 52/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.7500 - accuracy: 0.5139 - val_loss: 0.7321 - val_accuracy: 0.7500\n",
      "Epoch 53/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.7402 - accuracy: 0.6083 - val_loss: 0.7238 - val_accuracy: 0.6750\n",
      "Epoch 54/100\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.7359 - accuracy: 0.5750 - val_loss: 0.7272 - val_accuracy: 0.5500\n",
      "Epoch 55/100\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.7322 - accuracy: 0.5722 - val_loss: 0.7153 - val_accuracy: 0.6500\n",
      "Epoch 56/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.7291 - accuracy: 0.6056 - val_loss: 0.7121 - val_accuracy: 0.6000\n",
      "Epoch 57/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.7238 - accuracy: 0.5861 - val_loss: 0.7090 - val_accuracy: 0.5750\n",
      "Epoch 58/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.7239 - accuracy: 0.5722 - val_loss: 0.7080 - val_accuracy: 0.7000\n",
      "Epoch 59/100\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.7211 - accuracy: 0.5917 - val_loss: 0.7058 - val_accuracy: 0.7500\n",
      "Epoch 60/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.7185 - accuracy: 0.5639 - val_loss: 0.7076 - val_accuracy: 0.6500\n",
      "Epoch 61/100\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.7161 - accuracy: 0.5611 - val_loss: 0.7014 - val_accuracy: 0.9000\n",
      "Epoch 62/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.7129 - accuracy: 0.5778 - val_loss: 0.6953 - val_accuracy: 0.7500\n",
      "Epoch 63/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.7130 - accuracy: 0.5611 - val_loss: 0.6937 - val_accuracy: 0.7250\n",
      "Epoch 64/100\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.7050 - accuracy: 0.6222 - val_loss: 0.6951 - val_accuracy: 0.5250\n",
      "Epoch 65/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.7057 - accuracy: 0.6028 - val_loss: 0.6991 - val_accuracy: 0.6000\n",
      "Epoch 66/100\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.7063 - accuracy: 0.6194 - val_loss: 0.6910 - val_accuracy: 0.8250\n",
      "Epoch 67/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.7031 - accuracy: 0.6139 - val_loss: 0.6909 - val_accuracy: 0.6750\n",
      "Epoch 68/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.7010 - accuracy: 0.6417 - val_loss: 0.6874 - val_accuracy: 0.8750\n",
      "Epoch 69/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.6994 - accuracy: 0.6444 - val_loss: 0.6896 - val_accuracy: 0.5000\n",
      "Epoch 70/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.6985 - accuracy: 0.6194 - val_loss: 0.6826 - val_accuracy: 0.8000\n",
      "Epoch 71/100\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.6962 - accuracy: 0.6250 - val_loss: 0.6897 - val_accuracy: 0.5000\n",
      "Epoch 72/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.6950 - accuracy: 0.6056 - val_loss: 0.6840 - val_accuracy: 0.5750\n",
      "Epoch 73/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.6958 - accuracy: 0.6583 - val_loss: 0.6787 - val_accuracy: 0.7250\n",
      "Epoch 74/100\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.6913 - accuracy: 0.6861 - val_loss: 0.6818 - val_accuracy: 0.7000\n",
      "Epoch 75/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.6918 - accuracy: 0.6361 - val_loss: 0.6779 - val_accuracy: 0.8250\n",
      "Epoch 76/100\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.6885 - accuracy: 0.6639 - val_loss: 0.6757 - val_accuracy: 0.8250\n",
      "Epoch 77/100\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.6889 - accuracy: 0.6500 - val_loss: 0.6776 - val_accuracy: 0.6000\n",
      "Epoch 78/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.6870 - accuracy: 0.6750 - val_loss: 0.6743 - val_accuracy: 0.8250\n",
      "Epoch 79/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.6852 - accuracy: 0.7028 - val_loss: 0.6691 - val_accuracy: 0.6500\n",
      "Epoch 80/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.6850 - accuracy: 0.7111 - val_loss: 0.6745 - val_accuracy: 0.5750\n",
      "Epoch 81/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.6844 - accuracy: 0.6583 - val_loss: 0.6662 - val_accuracy: 0.7250\n",
      "Epoch 82/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.6852 - accuracy: 0.6361 - val_loss: 0.6675 - val_accuracy: 0.8250\n",
      "Epoch 83/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.6810 - accuracy: 0.6917 - val_loss: 0.6717 - val_accuracy: 0.7500\n",
      "Epoch 84/100\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.6788 - accuracy: 0.6333 - val_loss: 0.6694 - val_accuracy: 0.6000\n",
      "Epoch 85/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.6760 - accuracy: 0.6722 - val_loss: 0.6725 - val_accuracy: 0.6750\n",
      "Epoch 86/100\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.6780 - accuracy: 0.6361 - val_loss: 0.6625 - val_accuracy: 0.8750\n",
      "Epoch 87/100\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.6767 - accuracy: 0.6694 - val_loss: 0.6598 - val_accuracy: 0.6000\n",
      "Epoch 88/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.6742 - accuracy: 0.6250 - val_loss: 0.6568 - val_accuracy: 0.6750\n",
      "Epoch 89/100\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.6739 - accuracy: 0.7111 - val_loss: 0.6566 - val_accuracy: 0.8250\n",
      "Epoch 90/100\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.6713 - accuracy: 0.7111 - val_loss: 0.6602 - val_accuracy: 0.7500\n",
      "Epoch 91/100\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6689 - accuracy: 0.7167 - val_loss: 0.6573 - val_accuracy: 0.8000\n",
      "Epoch 92/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.6674 - accuracy: 0.7389 - val_loss: 0.6548 - val_accuracy: 0.8250\n",
      "Epoch 93/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.6692 - accuracy: 0.7306 - val_loss: 0.6580 - val_accuracy: 0.7750\n",
      "Epoch 94/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.6655 - accuracy: 0.6722 - val_loss: 0.6542 - val_accuracy: 0.8750\n",
      "Epoch 95/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.6678 - accuracy: 0.7639 - val_loss: 0.6528 - val_accuracy: 0.8250\n",
      "Epoch 96/100\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.6673 - accuracy: 0.7361 - val_loss: 0.6474 - val_accuracy: 0.8750\n",
      "Epoch 97/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.6633 - accuracy: 0.7111 - val_loss: 0.6521 - val_accuracy: 0.8250\n",
      "Epoch 98/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.6594 - accuracy: 0.7250 - val_loss: 0.6528 - val_accuracy: 0.6250\n",
      "Epoch 99/100\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.6605 - accuracy: 0.6778 - val_loss: 0.6436 - val_accuracy: 0.7500\n",
      "Epoch 100/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.6565 - accuracy: 0.7444 - val_loss: 0.6398 - val_accuracy: 0.8000\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7195 - accuracy: 0.4625\n",
      "Scores: [0.7195026278495789, 0.4625000059604645]\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "########################################################################\n",
    "# Experiment 4: can we classify a discretized histogram of sample data?\n",
    "#\n",
    "#\n",
    "#########################################################################\n",
    "'''\n",
    ")\n",
    "# let's try actual binning\n",
    "import collections\n",
    "\n",
    "def bin(row):\n",
    "    return np.histogram(row,bins=len(row),range=(0.0,1.0))[0]/float(len(row))\n",
    "\n",
    "print(\"Apply the histogram to all the data rows\")\n",
    "bdata = np.apply_along_axis(bin,1,wdata).astype(np.float32)\n",
    "blabels = wlabels\n",
    "\n",
    "# ensure we have our test data\n",
    "test_bdata = np.apply_along_axis(bin,1,test_wdata).astype(np.float32)\n",
    "test_blabels = test_wlabels\n",
    "\n",
    "# helper data \n",
    "enum_funcs = [\n",
    "    (LOGNORMAL,\"log normal\",lambda size: lognormal(size=size)),\n",
    "    (POWER,\"power\",lambda size: power(0.1,size=size)),\n",
    "    (NORM,\"normal\",lambda size: normal(size=size)),\n",
    "    (UNIFORM,\"uniforms\",lambda size: uniform(size=size)),\n",
    "]\n",
    "\n",
    "# uses enum_funcs to evaluate PER CLASS how well our classify operates\n",
    "def classify_test(bnet,ntests=1000):\n",
    "    for tup in enum_funcs:\n",
    "        enum, name, func = tup\n",
    "        lns = min_max_scale(func(size=(ntests,width))) #log normal\n",
    "        blns = np.apply_along_axis(bin,1,lns).astype(np.float32)\n",
    "        blns_labels = np.repeat(enum,ntests)\n",
    "        blns_labels.astype(np.int32)\n",
    "        classification = bnet.predict_classes(blns)\n",
    "        classified = theautil.classifications(classification,blns_labels)\n",
    "        print(\"Name:%s Tests:[%s] Count:%s -- Res:%s\" % (name,ntests, collections.Counter(classification),classified ))\n",
    "\n",
    "# train & valid\n",
    "btrain, bvalid = split_validation(90, bdata, blabels)\n",
    "\n",
    "encb = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "encb.fit(btrain[1].reshape(len(btrain[1]),1))\n",
    "btrain_y = encb.transform(btrain[1].reshape(len(btrain[1]),1))\n",
    "bvalid_y = encb.transform(bvalid[1].reshape(len(bvalid[1]),1))\n",
    "btest_y  = encb.transform(test_blabels.reshape(len(test_blabels),1))\n",
    "\n",
    "\n",
    "\n",
    "# similar network structure\n",
    "# bnet = theanets.Classifier([width,width/2,4])\n",
    "\n",
    "bnet = Sequential()\n",
    "bnet.add(Dense(width,input_shape=(width,),activation=\"sigmoid\"))\n",
    "bnet.add(Dense(int(width/4),activation=\"sigmoid\"))\n",
    "bnet.add(Dense(4,activation=\"softmax\"))\n",
    "bnet.compile(loss=\"categorical_crossentropy\", optimizer=SGD(lr=0.1), metrics=[\"accuracy\"])\n",
    "history = bnet.fit(btrain[0], btrain_y, validation_data=(bvalid[0], bvalid_y),\n",
    "\t            epochs=100, batch_size=16)\n",
    "score = bnet.evaluate(test_bdata, btest_y)\n",
    "print(\"Scores: %s\" % score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2973 - accuracy: 0.9528 - val_loss: 0.3181 - val_accuracy: 1.0000\n",
      "Epoch 2/100\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2884 - accuracy: 0.9556 - val_loss: 0.3343 - val_accuracy: 0.9750\n",
      "Epoch 3/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.2879 - accuracy: 0.9583 - val_loss: 0.3128 - val_accuracy: 0.9750\n",
      "Epoch 4/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.2778 - accuracy: 0.9639 - val_loss: 0.3284 - val_accuracy: 0.8750\n",
      "Epoch 5/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2812 - accuracy: 0.9528 - val_loss: 0.3057 - val_accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2858 - accuracy: 0.9639 - val_loss: 0.3138 - val_accuracy: 0.9000\n",
      "Epoch 7/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.2725 - accuracy: 0.9639 - val_loss: 0.3025 - val_accuracy: 0.9750\n",
      "Epoch 8/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2697 - accuracy: 0.9694 - val_loss: 0.3179 - val_accuracy: 0.8750\n",
      "Epoch 9/100\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.2689 - accuracy: 0.9667 - val_loss: 0.2953 - val_accuracy: 0.9750\n",
      "Epoch 10/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2667 - accuracy: 0.9667 - val_loss: 0.2921 - val_accuracy: 0.9500\n",
      "Epoch 11/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2614 - accuracy: 0.9639 - val_loss: 0.2796 - val_accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2557 - accuracy: 0.9694 - val_loss: 0.2944 - val_accuracy: 0.9250\n",
      "Epoch 13/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2600 - accuracy: 0.9556 - val_loss: 0.2862 - val_accuracy: 0.9500\n",
      "Epoch 14/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2526 - accuracy: 0.9778 - val_loss: 0.2799 - val_accuracy: 0.9500\n",
      "Epoch 15/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2453 - accuracy: 0.9667 - val_loss: 0.2781 - val_accuracy: 0.9750\n",
      "Epoch 16/100\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.2419 - accuracy: 0.9806 - val_loss: 0.2662 - val_accuracy: 0.9750\n",
      "Epoch 17/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2429 - accuracy: 0.9611 - val_loss: 0.2587 - val_accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2383 - accuracy: 0.9750 - val_loss: 0.2546 - val_accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2348 - accuracy: 0.9750 - val_loss: 0.2525 - val_accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2301 - accuracy: 0.9806 - val_loss: 0.2538 - val_accuracy: 0.9750\n",
      "Epoch 21/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2282 - accuracy: 0.9611 - val_loss: 0.2562 - val_accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.2281 - accuracy: 0.9694 - val_loss: 0.2711 - val_accuracy: 0.9500\n",
      "Epoch 23/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2211 - accuracy: 0.9806 - val_loss: 0.2402 - val_accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.2180 - accuracy: 0.9833 - val_loss: 0.2332 - val_accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2190 - accuracy: 0.9806 - val_loss: 0.2335 - val_accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.2123 - accuracy: 0.9861 - val_loss: 0.2349 - val_accuracy: 0.9750\n",
      "Epoch 27/100\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.2084 - accuracy: 0.9806 - val_loss: 0.2240 - val_accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.2057 - accuracy: 0.9861 - val_loss: 0.2268 - val_accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.2034 - accuracy: 0.9833 - val_loss: 0.2211 - val_accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2003 - accuracy: 0.9833 - val_loss: 0.2159 - val_accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.1946 - accuracy: 0.9861 - val_loss: 0.2104 - val_accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.1934 - accuracy: 0.9889 - val_loss: 0.2169 - val_accuracy: 0.9750\n",
      "Epoch 33/100\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.1914 - accuracy: 0.9750 - val_loss: 0.2153 - val_accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.1891 - accuracy: 0.9833 - val_loss: 0.2036 - val_accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.1855 - accuracy: 0.9889 - val_loss: 0.1998 - val_accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.1828 - accuracy: 0.9889 - val_loss: 0.1990 - val_accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.1784 - accuracy: 0.9944 - val_loss: 0.1995 - val_accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.1778 - accuracy: 0.9944 - val_loss: 0.1988 - val_accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1773 - accuracy: 0.9889 - val_loss: 0.1910 - val_accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.1758 - accuracy: 0.9861 - val_loss: 0.1902 - val_accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.1721 - accuracy: 0.9889 - val_loss: 0.1853 - val_accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.1716 - accuracy: 0.9806 - val_loss: 0.1799 - val_accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.1703 - accuracy: 0.9861 - val_loss: 0.1872 - val_accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.1660 - accuracy: 0.9861 - val_loss: 0.1750 - val_accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.1616 - accuracy: 0.9889 - val_loss: 0.1790 - val_accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.1619 - accuracy: 0.9861 - val_loss: 0.1719 - val_accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.1609 - accuracy: 0.9889 - val_loss: 0.1678 - val_accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.1567 - accuracy: 0.9889 - val_loss: 0.1748 - val_accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1541 - accuracy: 0.9889 - val_loss: 0.1668 - val_accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.1540 - accuracy: 0.9889 - val_loss: 0.1614 - val_accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1503 - accuracy: 0.9889 - val_loss: 0.1730 - val_accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1477 - accuracy: 0.9944 - val_loss: 0.1778 - val_accuracy: 0.9750\n",
      "Epoch 53/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.1478 - accuracy: 0.9833 - val_loss: 0.1610 - val_accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1434 - accuracy: 0.9889 - val_loss: 0.1684 - val_accuracy: 0.9750\n",
      "Epoch 55/100\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.1457 - accuracy: 0.9917 - val_loss: 0.1553 - val_accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.1384 - accuracy: 0.9944 - val_loss: 0.1585 - val_accuracy: 0.9750\n",
      "Epoch 57/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1389 - accuracy: 0.99 - 0s 8ms/step - loss: 0.1396 - accuracy: 0.9972 - val_loss: 0.1561 - val_accuracy: 0.9750\n",
      "Epoch 58/100\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.1353 - accuracy: 0.9917 - val_loss: 0.1449 - val_accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.1362 - accuracy: 0.9917 - val_loss: 0.1517 - val_accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.1339 - accuracy: 0.9889 - val_loss: 0.1439 - val_accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.1320 - accuracy: 0.9972 - val_loss: 0.1417 - val_accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.1323 - accuracy: 0.9944 - val_loss: 0.1413 - val_accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.1312 - accuracy: 0.9944 - val_loss: 0.1353 - val_accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.1272 - accuracy: 0.9944 - val_loss: 0.1332 - val_accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.1255 - accuracy: 0.9917 - val_loss: 0.1362 - val_accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.1235 - accuracy: 0.9972 - val_loss: 0.1335 - val_accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.1214 - accuracy: 0.9944 - val_loss: 0.1347 - val_accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.1209 - accuracy: 0.9944 - val_loss: 0.1388 - val_accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.1187 - accuracy: 0.9944 - val_loss: 0.1272 - val_accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.1193 - accuracy: 0.9889 - val_loss: 0.1256 - val_accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.1155 - accuracy: 0.9972 - val_loss: 0.1245 - val_accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.1131 - accuracy: 0.9944 - val_loss: 0.1630 - val_accuracy: 0.9750\n",
      "Epoch 73/100\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.1204 - accuracy: 0.9917 - val_loss: 0.1255 - val_accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.1148 - accuracy: 0.9944 - val_loss: 0.1201 - val_accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.1111 - accuracy: 0.9972 - val_loss: 0.1201 - val_accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.1125 - accuracy: 0.9944 - val_loss: 0.1196 - val_accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.1100 - accuracy: 0.9889 - val_loss: 0.1174 - val_accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.1071 - accuracy: 0.9972 - val_loss: 0.1180 - val_accuracy: 0.9750\n",
      "Epoch 79/100\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.1066 - accuracy: 0.9917 - val_loss: 0.1257 - val_accuracy: 0.9750\n",
      "Epoch 80/100\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.1060 - accuracy: 0.9944 - val_loss: 0.1218 - val_accuracy: 0.9750\n",
      "Epoch 81/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.1054 - accuracy: 0.9944 - val_loss: 0.1115 - val_accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.1032 - accuracy: 0.9972 - val_loss: 0.1123 - val_accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.1028 - accuracy: 0.9972 - val_loss: 0.1104 - val_accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.1010 - accuracy: 0.9944 - val_loss: 0.1121 - val_accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0994 - accuracy: 0.9944 - val_loss: 0.1285 - val_accuracy: 0.9750\n",
      "Epoch 86/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.1010 - accuracy: 0.9917 - val_loss: 0.1046 - val_accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0983 - accuracy: 0.9944 - val_loss: 0.1040 - val_accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0975 - accuracy: 0.9972 - val_loss: 0.1086 - val_accuracy: 0.9750\n",
      "Epoch 89/100\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.0969 - accuracy: 0.9917 - val_loss: 0.1047 - val_accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0950 - accuracy: 0.9972 - val_loss: 0.1015 - val_accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.0939 - accuracy: 0.9944 - val_loss: 0.1020 - val_accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0958 - accuracy: 0.9944 - val_loss: 0.1020 - val_accuracy: 0.9750\n",
      "Epoch 93/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0934 - accuracy: 0.9917 - val_loss: 0.1057 - val_accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0943 - accuracy: 0.9944 - val_loss: 0.1019 - val_accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.0899 - accuracy: 0.9972 - val_loss: 0.0986 - val_accuracy: 0.9750\n",
      "Epoch 96/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0903 - accuracy: 1.0000 - val_loss: 0.0971 - val_accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.0889 - accuracy: 0.9944 - val_loss: 0.0971 - val_accuracy: 0.9750\n",
      "Epoch 98/100\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.0906 - accuracy: 0.9972 - val_loss: 0.1022 - val_accuracy: 0.9750\n",
      "Epoch 99/100\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0890 - accuracy: 1.0000 - val_loss: 0.0956 - val_accuracy: 0.9750\n",
      "Epoch 100/100\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0873 - accuracy: 0.9944 - val_loss: 0.0973 - val_accuracy: 0.9750\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.1253 - accuracy: 0.9725\n",
      "Scores: [0.1252535879611969, 0.9725000262260437]\n"
     ]
    }
   ],
   "source": [
    "history = bnet.fit(btrain[0], btrain_y, validation_data=(bvalid[0], bvalid_y),\n",
    "\t            epochs=100, batch_size=16)\n",
    "score = bnet.evaluate(test_bdata, btest_y)\n",
    "print(\"Scores: %s\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tp', 99), ('tn', 90), ('fp', 10), ('fn', 1)]\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.1253 - accuracy: 0.9725\n",
      "Scores: [0.1252535879611969, 0.9725000262260437]\n",
      "Name:log normal Tests:[1000] Count:Counter({0: 935, 1: 65}) -- Res:[('tp', 0), ('tn', 935), ('fp', 65), ('fn', 0)]\n",
      "Name:power Tests:[1000] Count:Counter({1: 987, 0: 13}) -- Res:[('tp', 987), ('tn', 0), ('fp', 0), ('fn', 13)]\n",
      "Name:normal Tests:[1000] Count:Counter({2: 1000}) -- Res:[('tp', 0), ('tn', 0), ('fp', 0), ('fn', 0)]\n",
      "Name:uniforms Tests:[1000] Count:Counter({3: 996, 2: 4}) -- Res:[('tp', 0), ('tn', 0), ('fp', 0), ('fn', 0)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "classify = bnet.predict_classes(test_bdata)\n",
    "print(theautil.classifications(classify,test_blabels))\n",
    "score = bnet.evaluate(test_bdata, btest_y)\n",
    "print(\"Scores: %s\" % score)\n",
    "\n",
    "classify_test(bnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation: Inputs\n",
    "\n",
    "* For discrete values consider discrete inputs neurons. E.g. if you have 3 letters are your input you should have 3 * 26 input neurons. \n",
    "* Each neuron is \"one-hot\" -- 1 neuron is set to 1 to indicate that 1 discerete value. \n",
    "* An input of AAA would be: \n",
    "  * 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
    "* ZZZ would be \n",
    "  * 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation: Inputs\n",
    "\n",
    "* For groups of elements consider representing them as their counts.\n",
    "* E.g. 3 cats, 4 dogs, 1 car as: 3 4 1 on 3 input neurons.\n",
    "* Neural networks work well with distributions as inputs and distributions as outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Representation: Words\n",
    "\n",
    "* Words can be represented as word counts where by your vector is the count of each word per document -- you might have a large vocabulary so watch out!\n",
    "* n-grams are popular too with one-hot encoding\n",
    "* Embeddings (a dense vector representation) are popular too. Autoencoded words!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Representaiton: Images\n",
    "\n",
    "* Each neuron can represent a pixel represented from 0 to 1\n",
    "* You can have images as output too!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Representation: Outputs\n",
    "\n",
    "* Do not ask the neural network to distingush discrete values on 1 neuron. Don't expect 1 neuron to output 0.25 for A and 0.9 for B and 1.0 for C. Use 3 neurons!\n",
    "* Distribution outputs are good\n",
    "* Interpretting the output is fine for regression problems\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning\n",
    "\n",
    "* The parameters you chose were probably not correct!\n",
    "* You could grid search, that is try all combinations. But that takes a lot of time.\n",
    "* We want to get engage in hyper-parameter tuning.\n",
    "  * We want to find good parameters for our network that perform well.\n",
    "* Grid Search\n",
    "  * Step 1: choose the parameter space\n",
    "  * Step 2: choose a method of selecting parameters\n",
    "  * Step 3: get next combination of parameters\n",
    "  * Step 4: evaluate the parameters\n",
    "  * Step 5: If current performance is better than prior performances keep this set of parameters\n",
    "  * Step 6: goto step 3 until all parameter combinations are exhausted.\n",
    "  * Step 7: report results\n",
    "* Random Search\n",
    "  * Step 1: choose the parameter space\n",
    "  * Step 2: choose a method of selecting parameters\n",
    "  * Step 3: randomly choose parameters\n",
    "  * Step 4: Evaluate the parameters\n",
    "  * Step 5: If current performance is better than prior performances keep this set of parameters\n",
    "  * Step 6: goto step 3 until satisfied (N iterations or M seconds)\n",
    "  * Step 7: report results\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "########################################################################\n",
      "# Experiment 5: Can we tune the binned data?\n",
      "#\n",
      "#\n",
      "#########################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's tune\n",
    "print('''\n",
    "########################################################################\n",
    "# Experiment 5: Can we tune the binned data?\n",
    "#\n",
    "#\n",
    "#########################################################################\n",
    "'''\n",
    ")\n",
    "\n",
    "\n",
    "import Search\n",
    "\n",
    "# 1 repetition\n",
    "state = {\"reps\":1}\n",
    "params = {\"batch_size\":[1,4,8,16,32,64],\n",
    "          \"lr\":[1.0,0.1,0.01,0.001,0.0001],\n",
    "          \"activation\":[\"sigmoid\",\"tanh\",\"relu\"],\n",
    "          \"optimizer\":[\"SGD\",\"Adam\"],\n",
    "          \"epochs\":[25],\n",
    "          \"arch\":[\n",
    "              [width],\n",
    "              [width,width],\n",
    "              [width,int(width/4)],\n",
    "              [2*width,width],\n",
    "              [int(width/4),int(width/8)],\n",
    "              [int(width/4)]]\n",
    "          }\n",
    "def get_optimizer(x):\n",
    "    if x == \"Adam\":\n",
    "        return Adam\n",
    "    return SGD\n",
    "    \n",
    "def f(state,params):\n",
    "    bnet = Sequential()\n",
    "    arch = params[\"arch\"]\n",
    "    bnet.add(Dense(arch[0],input_shape=(width,),activation=params[\"activation\"]))\n",
    "    for layer in arch[1:]:\n",
    "        bnet.add(Dense(int(layer),activation=params[\"activation\"]))\n",
    "    bnet.add(Dense(4,activation=\"softmax\"))\n",
    "    optimizer = get_optimizer(params[\"optimizer\"])\n",
    "    bnet.compile(loss=\"categorical_crossentropy\",\n",
    "                 optimizer=optimizer(lr=params[\"lr\"]), metrics=[\"accuracy\"])\n",
    "    history = bnet.fit(btrain[0], btrain_y,\n",
    "                       validation_data=(bvalid[0], bvalid_y),\n",
    "\t               epochs=params[\"epochs\"], batch_size=params[\"batch_size\"])\n",
    "    classify = bnet.predict_classes(test_bdata)\n",
    "    print(theautil.classifications(classify,test_blabels))\n",
    "    score = bnet.evaluate(test_bdata, btest_y)\n",
    "    print(\"Scores: %s\" % score)\n",
    "    return score[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 1.3951 - accuracy: 0.2194 - val_loss: 1.3131 - val_accuracy: 0.6000\n",
      "Epoch 2/25\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 1.2674 - accuracy: 0.5750 - val_loss: 1.1902 - val_accuracy: 0.6750\n",
      "Epoch 3/25\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 1.1455 - accuracy: 0.5861 - val_loss: 1.0651 - val_accuracy: 0.6000\n",
      "Epoch 4/25\n",
      "90/90 [==============================] - 1s 11ms/step - loss: 1.0223 - accuracy: 0.5500 - val_loss: 0.9570 - val_accuracy: 0.7000\n",
      "Epoch 5/25\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.9157 - accuracy: 0.6444 - val_loss: 0.8637 - val_accuracy: 0.8000\n",
      "Epoch 6/25\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.8354 - accuracy: 0.6972 - val_loss: 0.7974 - val_accuracy: 0.6000\n",
      "Epoch 7/25\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.7816 - accuracy: 0.7167 - val_loss: 0.7556 - val_accuracy: 0.6250\n",
      "Epoch 8/25\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.7458 - accuracy: 0.7333 - val_loss: 0.7228 - val_accuracy: 0.7750\n",
      "Epoch 9/25\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.7184 - accuracy: 0.7833 - val_loss: 0.7005 - val_accuracy: 0.7250\n",
      "Epoch 10/25\n",
      "90/90 [==============================] - 1s 11ms/step - loss: 0.6999 - accuracy: 0.8000 - val_loss: 0.6841 - val_accuracy: 0.7250\n",
      "Epoch 11/25\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.6829 - accuracy: 0.7556 - val_loss: 0.6717 - val_accuracy: 0.8750\n",
      "Epoch 12/25\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.6699 - accuracy: 0.8194 - val_loss: 0.6587 - val_accuracy: 0.9250\n",
      "Epoch 13/25\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.6563 - accuracy: 0.8667 - val_loss: 0.6455 - val_accuracy: 0.8750\n",
      "Epoch 14/25\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.6457 - accuracy: 0.8667 - val_loss: 0.6360 - val_accuracy: 0.9000\n",
      "Epoch 15/25\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.6334 - accuracy: 0.9139 - val_loss: 0.6264 - val_accuracy: 0.8750\n",
      "Epoch 16/25\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.6244 - accuracy: 0.8778 - val_loss: 0.6186 - val_accuracy: 0.9500\n",
      "Epoch 17/25\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.6123 - accuracy: 0.9111 - val_loss: 0.6069 - val_accuracy: 0.9750\n",
      "Epoch 18/25\n",
      "90/90 [==============================] - 1s 11ms/step - loss: 0.6005 - accuracy: 0.9333 - val_loss: 0.5984 - val_accuracy: 0.9000\n",
      "Epoch 19/25\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.5822 - accuracy: 0.8694 - val_loss: 0.5862 - val_accuracy: 0.7250\n",
      "Epoch 20/25\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.5783 - accuracy: 0.9056 - val_loss: 0.5709 - val_accuracy: 0.9750\n",
      "Epoch 21/25\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.5642 - accuracy: 0.9583 - val_loss: 0.5601 - val_accuracy: 0.9000\n",
      "Epoch 22/25\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.5482 - accuracy: 0.9139 - val_loss: 0.5592 - val_accuracy: 0.7250\n",
      "Epoch 23/25\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.5359 - accuracy: 0.9000 - val_loss: 0.5410 - val_accuracy: 0.9750\n",
      "Epoch 24/25\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.5251 - accuracy: 0.9667 - val_loss: 0.5234 - val_accuracy: 0.9750\n",
      "Epoch 25/25\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.5109 - accuracy: 0.9694 - val_loss: 0.5111 - val_accuracy: 0.9750\n",
      "[('tp', 98), ('tn', 94), ('fp', 6), ('fn', 2)]\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.5115 - accuracy: 0.9750\n",
      "Scores: [0.5114760994911194, 0.9750000238418579]\n",
      "Epoch 1/25\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 1.4403 - accuracy: 0.2611 - val_loss: 1.4793 - val_accuracy: 0.1500\n",
      "Epoch 2/25\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 1.3912 - accuracy: 0.2611 - val_loss: 1.4217 - val_accuracy: 0.1500\n",
      "Epoch 3/25\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 1.3674 - accuracy: 0.2972 - val_loss: 1.3816 - val_accuracy: 0.3500\n",
      "Epoch 4/25\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 1.3481 - accuracy: 0.4583 - val_loss: 1.3607 - val_accuracy: 0.3500\n",
      "Epoch 5/25\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 1.3313 - accuracy: 0.6083 - val_loss: 1.3426 - val_accuracy: 0.3500\n",
      "Epoch 6/25\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.3134 - accuracy: 0.5194 - val_loss: 1.3218 - val_accuracy: 0.4750\n",
      "Epoch 7/25\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 1.2935 - accuracy: 0.5778 - val_loss: 1.2999 - val_accuracy: 0.6500\n",
      "Epoch 8/25\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 1.2715 - accuracy: 0.7361 - val_loss: 1.2738 - val_accuracy: 0.6500\n",
      "Epoch 9/25\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 1.2458 - accuracy: 0.6889 - val_loss: 1.2507 - val_accuracy: 0.6500\n",
      "Epoch 10/25\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.2172 - accuracy: 0.6361 - val_loss: 1.2235 - val_accuracy: 0.6750\n",
      "Epoch 11/25\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 1.1858 - accuracy: 0.7167 - val_loss: 1.1879 - val_accuracy: 0.6750\n",
      "Epoch 12/25\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 1.1512 - accuracy: 0.7639 - val_loss: 1.1564 - val_accuracy: 0.7000\n",
      "Epoch 13/25\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 1.1144 - accuracy: 0.7361 - val_loss: 1.1196 - val_accuracy: 0.6750\n",
      "Epoch 14/25\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 1.0764 - accuracy: 0.7028 - val_loss: 1.0754 - val_accuracy: 0.7000\n",
      "Epoch 15/25\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 1.0365 - accuracy: 0.7583 - val_loss: 1.0399 - val_accuracy: 0.7000\n",
      "Epoch 16/25\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.9967 - accuracy: 0.7667 - val_loss: 1.0006 - val_accuracy: 0.7000\n",
      "Epoch 17/25\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.9587 - accuracy: 0.8167 - val_loss: 0.9640 - val_accuracy: 0.7000\n",
      "Epoch 18/25\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.9211 - accuracy: 0.7611 - val_loss: 0.9274 - val_accuracy: 0.7000\n",
      "Epoch 19/25\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.8848 - accuracy: 0.7917 - val_loss: 0.8919 - val_accuracy: 0.7000\n",
      "Epoch 20/25\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.8503 - accuracy: 0.7583 - val_loss: 0.8607 - val_accuracy: 0.7000\n",
      "Epoch 21/25\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.8192 - accuracy: 0.7972 - val_loss: 0.8302 - val_accuracy: 0.7000\n",
      "Epoch 22/25\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.7893 - accuracy: 0.8278 - val_loss: 0.8035 - val_accuracy: 0.7000\n",
      "Epoch 23/25\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.7613 - accuracy: 0.8139 - val_loss: 0.7786 - val_accuracy: 0.7750\n",
      "Epoch 24/25\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.7365 - accuracy: 0.8194 - val_loss: 0.7564 - val_accuracy: 0.7000\n",
      "Epoch 25/25\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.7116 - accuracy: 0.8361 - val_loss: 0.7335 - val_accuracy: 0.8000\n",
      "[('tp', 43), ('tn', 100), ('fp', 0), ('fn', 57)]\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.7100 - accuracy: 0.8575\n",
      "Scores: [0.7099908590316772, 0.8575000166893005]\n",
      "Epoch 1/25\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 1.3790 - accuracy: 0.2556 - val_loss: 1.3689 - val_accuracy: 0.3500\n",
      "Epoch 2/25\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 1.3784 - accuracy: 0.2556 - val_loss: 1.3684 - val_accuracy: 0.3500\n",
      "Epoch 3/25\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 1.3779 - accuracy: 0.2611 - val_loss: 1.3679 - val_accuracy: 0.3500\n",
      "Epoch 4/25\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 1.3774 - accuracy: 0.2667 - val_loss: 1.3674 - val_accuracy: 0.3500\n",
      "Epoch 5/25\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 1.3768 - accuracy: 0.2694 - val_loss: 1.3668 - val_accuracy: 0.3500\n",
      "Epoch 6/25\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 1.3763 - accuracy: 0.2694 - val_loss: 1.3663 - val_accuracy: 0.3500\n",
      "Epoch 7/25\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 1.3758 - accuracy: 0.2722 - val_loss: 1.3659 - val_accuracy: 0.3500\n",
      "Epoch 8/25\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 1.3752 - accuracy: 0.2778 - val_loss: 1.3653 - val_accuracy: 0.3750\n",
      "Epoch 9/25\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 1.3747 - accuracy: 0.2778 - val_loss: 1.3648 - val_accuracy: 0.3750\n",
      "Epoch 10/25\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1.3742 - accuracy: 0.2778 - val_loss: 1.3643 - val_accuracy: 0.3750\n",
      "Epoch 11/25\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1.3737 - accuracy: 0.2778 - val_loss: 1.3638 - val_accuracy: 0.3750\n",
      "Epoch 12/25\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 1.3732 - accuracy: 0.2806 - val_loss: 1.3633 - val_accuracy: 0.3750\n",
      "Epoch 13/25\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 1.3727 - accuracy: 0.2833 - val_loss: 1.3628 - val_accuracy: 0.3750\n",
      "Epoch 14/25\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 1.3722 - accuracy: 0.2861 - val_loss: 1.3624 - val_accuracy: 0.3750\n",
      "Epoch 15/25\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 1.3717 - accuracy: 0.2889 - val_loss: 1.3619 - val_accuracy: 0.3750\n",
      "Epoch 16/25\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 1.3712 - accuracy: 0.2944 - val_loss: 1.3614 - val_accuracy: 0.3750\n",
      "Epoch 17/25\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1.3707 - accuracy: 0.3000 - val_loss: 1.3610 - val_accuracy: 0.3750\n",
      "Epoch 18/25\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1.3702 - accuracy: 0.3028 - val_loss: 1.3606 - val_accuracy: 0.3750\n",
      "Epoch 19/25\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 1.3697 - accuracy: 0.3056 - val_loss: 1.3601 - val_accuracy: 0.3750\n",
      "Epoch 20/25\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 1.3692 - accuracy: 0.3111 - val_loss: 1.3597 - val_accuracy: 0.3750\n",
      "Epoch 21/25\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 1.3687 - accuracy: 0.3139 - val_loss: 1.3592 - val_accuracy: 0.3750\n",
      "Epoch 22/25\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.3682 - accuracy: 0.3278 - val_loss: 1.3587 - val_accuracy: 0.3750\n",
      "Epoch 23/25\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 1.3677 - accuracy: 0.3333 - val_loss: 1.3582 - val_accuracy: 0.3750\n",
      "Epoch 24/25\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.3671 - accuracy: 0.3472 - val_loss: 1.3578 - val_accuracy: 0.3750\n",
      "Epoch 25/25\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1.3666 - accuracy: 0.3472 - val_loss: 1.3573 - val_accuracy: 0.3750\n",
      "[('tp', 1), ('tn', 96), ('fp', 3), ('fn', 99)]\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1.3723 - accuracy: 0.3275\n",
      "Scores: [1.372337818145752, 0.32749998569488525]\n",
      "Epoch 1/25\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 10.3787 - accuracy: 0.3667 - val_loss: 7.8998 - val_accuracy: 0.4750\n",
      "Epoch 2/25\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 4.4352 - accuracy: 0.6917 - val_loss: 2.4019 - val_accuracy: 0.5500\n",
      "Epoch 3/25\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1.4801 - accuracy: 0.8083 - val_loss: 0.1920 - val_accuracy: 0.9750\n",
      "Epoch 4/25\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.0207 - accuracy: 0.9889 - val_loss: 0.0753 - val_accuracy: 0.9500\n",
      "Epoch 5/25\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0081 - accuracy: 0.9972 - val_loss: 0.1339 - val_accuracy: 0.9750\n",
      "Epoch 6/25\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0065 - accuracy: 0.9972 - val_loss: 0.1126 - val_accuracy: 0.9750\n",
      "Epoch 7/25\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 8.2450e-05 - accuracy: 1.0000 - val_loss: 0.1459 - val_accuracy: 0.9250\n",
      "Epoch 8/25\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 0.0025 - accuracy: 0.9972 - val_loss: 0.1264 - val_accuracy: 0.9750\n",
      "Epoch 9/25\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0062 - accuracy: 0.9944 - val_loss: 0.1267 - val_accuracy: 0.9500\n",
      "Epoch 10/25\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0196 - accuracy: 0.9944 - val_loss: 0.1477 - val_accuracy: 0.9750\n",
      "Epoch 11/25\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.0034 - accuracy: 0.9972 - val_loss: 0.1039 - val_accuracy: 0.9500\n",
      "Epoch 12/25\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0035 - accuracy: 0.9972 - val_loss: 0.1500 - val_accuracy: 0.9250\n",
      "Epoch 13/25\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 2.1485e-04 - accuracy: 1.0000 - val_loss: 0.2787 - val_accuracy: 0.9750\n",
      "Epoch 14/25\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.1660 - val_accuracy: 0.9250\n",
      "Epoch 15/25\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 4.8461e-05 - accuracy: 1.0000 - val_loss: 0.4001 - val_accuracy: 0.9500\n",
      "Epoch 16/25\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.2032 - val_accuracy: 0.9500\n",
      "Epoch 17/25\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.2994 - val_accuracy: 0.9250\n",
      "Epoch 18/25\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.0066 - accuracy: 0.9972 - val_loss: 0.1980 - val_accuracy: 0.9500\n",
      "Epoch 19/25\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.0086 - accuracy: 0.9972 - val_loss: 0.2899 - val_accuracy: 0.9500\n",
      "Epoch 20/25\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.0958 - accuracy: 0.9861 - val_loss: 0.4150 - val_accuracy: 0.9750\n",
      "Epoch 21/25\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.7283 - accuracy: 0.9306 - val_loss: 0.1909 - val_accuracy: 0.9750\n",
      "Epoch 22/25\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.1579 - accuracy: 0.9833 - val_loss: 0.5099 - val_accuracy: 0.9750\n",
      "Epoch 23/25\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.1139 - accuracy: 0.9750 - val_loss: 0.6891 - val_accuracy: 0.9500\n",
      "Epoch 24/25\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0097 - accuracy: 0.9944 - val_loss: 0.4252 - val_accuracy: 0.9750\n",
      "Epoch 25/25\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.0676 - accuracy: 0.9917 - val_loss: 0.3868 - val_accuracy: 0.9500\n",
      "[('tp', 97), ('tn', 99), ('fp', 1), ('fn', 3)]\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.1024 - accuracy: 0.9875\n",
      "Scores: [0.10237475484609604, 0.987500011920929]\n",
      "Epoch 1/25\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 1.9369 - accuracy: 0.2500 - val_loss: 1.3725 - val_accuracy: 0.3250\n",
      "Epoch 2/25\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 1.4154 - accuracy: 0.2278 - val_loss: 1.4561 - val_accuracy: 0.1500\n",
      "Epoch 3/25\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 1.4186 - accuracy: 0.2167 - val_loss: 1.4215 - val_accuracy: 0.1500\n",
      "Epoch 4/25\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 1.3903 - accuracy: 0.2972 - val_loss: 1.4606 - val_accuracy: 0.1500\n",
      "Epoch 5/25\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 1.4112 - accuracy: 0.2722 - val_loss: 1.3877 - val_accuracy: 0.4750\n",
      "Epoch 6/25\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 1.3935 - accuracy: 0.2806 - val_loss: 1.3866 - val_accuracy: 0.3250\n",
      "Epoch 7/25\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1.3953 - accuracy: 0.3389 - val_loss: 1.3552 - val_accuracy: 0.3250\n",
      "Epoch 8/25\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 1.3922 - accuracy: 0.2500 - val_loss: 1.3686 - val_accuracy: 0.3250\n",
      "Epoch 9/25\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1.3895 - accuracy: 0.2361 - val_loss: 1.3849 - val_accuracy: 0.1500\n",
      "Epoch 10/25\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 1.3937 - accuracy: 0.2833 - val_loss: 1.4168 - val_accuracy: 0.1500\n",
      "Epoch 11/25\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 1.3824 - accuracy: 0.2472 - val_loss: 1.3563 - val_accuracy: 0.3250\n",
      "Epoch 12/25\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1.3756 - accuracy: 0.3389 - val_loss: 1.3667 - val_accuracy: 0.3250\n",
      "Epoch 13/25\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 1.3577 - accuracy: 0.3722 - val_loss: 1.5386 - val_accuracy: 0.1500\n",
      "Epoch 14/25\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.3710 - accuracy: 0.2750 - val_loss: 1.3728 - val_accuracy: 0.1500\n",
      "Epoch 15/25\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 1.3475 - accuracy: 0.2667 - val_loss: 1.4279 - val_accuracy: 0.2000\n",
      "Epoch 16/25\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 1.3378 - accuracy: 0.2833 - val_loss: 1.2780 - val_accuracy: 0.3250\n",
      "Epoch 17/25\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 1.3399 - accuracy: 0.2583 - val_loss: 1.2706 - val_accuracy: 0.3250\n",
      "Epoch 18/25\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 1.2966 - accuracy: 0.3611 - val_loss: 1.1984 - val_accuracy: 0.5250\n",
      "Epoch 19/25\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 1.2605 - accuracy: 0.4056 - val_loss: 1.1895 - val_accuracy: 0.3250\n",
      "Epoch 20/25\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 1.3156 - accuracy: 0.2583 - val_loss: 1.3501 - val_accuracy: 0.2000\n",
      "Epoch 21/25\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 1.1595 - accuracy: 0.3861 - val_loss: 1.6651 - val_accuracy: 0.1500\n",
      "Epoch 22/25\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 1.4883 - accuracy: 0.3278 - val_loss: 1.0774 - val_accuracy: 0.4750\n",
      "Epoch 23/25\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 1.0885 - accuracy: 0.4444 - val_loss: 1.0009 - val_accuracy: 0.5250\n",
      "Epoch 24/25\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1.0216 - accuracy: 0.4639 - val_loss: 0.9899 - val_accuracy: 0.4000\n",
      "Epoch 25/25\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 1.1951 - accuracy: 0.3111 - val_loss: 0.9277 - val_accuracy: 0.4750\n",
      "[('tp', 0), ('tn', 96), ('fp', 4), ('fn', 100)]\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1.0254 - accuracy: 0.4750\n",
      "Scores: [1.0253547430038452, 0.4749999940395355]\n",
      "Epoch 1/25\n",
      "360/360 [==============================] - 3s 9ms/step - loss: 1.2277 - accuracy: 0.4556 - val_loss: 1.0392 - val_accuracy: 0.4750\n",
      "Epoch 2/25\n",
      "360/360 [==============================] - 3s 7ms/step - loss: 0.9562 - accuracy: 0.5000 - val_loss: 0.8441 - val_accuracy: 0.4750\n",
      "Epoch 3/25\n",
      "360/360 [==============================] - 3s 7ms/step - loss: 0.8090 - accuracy: 0.5194 - val_loss: 0.7654 - val_accuracy: 0.4750\n",
      "Epoch 4/25\n",
      "360/360 [==============================] - 3s 9ms/step - loss: 0.7532 - accuracy: 0.5167 - val_loss: 0.7260 - val_accuracy: 0.4750\n",
      "Epoch 5/25\n",
      "360/360 [==============================] - 3s 8ms/step - loss: 0.7295 - accuracy: 0.5111 - val_loss: 0.7131 - val_accuracy: 0.4750\n",
      "Epoch 6/25\n",
      "360/360 [==============================] - 3s 8ms/step - loss: 0.7139 - accuracy: 0.5333 - val_loss: 0.6921 - val_accuracy: 0.5250\n",
      "Epoch 7/25\n",
      "360/360 [==============================] - 3s 8ms/step - loss: 0.7065 - accuracy: 0.5472 - val_loss: 0.7044 - val_accuracy: 0.4750\n",
      "Epoch 8/25\n",
      "360/360 [==============================] - 3s 8ms/step - loss: 0.6972 - accuracy: 0.5583 - val_loss: 0.6891 - val_accuracy: 0.6000\n",
      "Epoch 9/25\n",
      "360/360 [==============================] - 3s 8ms/step - loss: 0.6875 - accuracy: 0.6000 - val_loss: 0.7007 - val_accuracy: 0.4750\n",
      "Epoch 10/25\n",
      "360/360 [==============================] - 3s 8ms/step - loss: 0.6874 - accuracy: 0.5972 - val_loss: 0.6771 - val_accuracy: 0.6000\n",
      "Epoch 11/25\n",
      "360/360 [==============================] - 3s 8ms/step - loss: 0.6651 - accuracy: 0.6639 - val_loss: 0.6658 - val_accuracy: 0.5500\n",
      "Epoch 12/25\n",
      "360/360 [==============================] - 2s 7ms/step - loss: 0.6623 - accuracy: 0.6556 - val_loss: 0.6429 - val_accuracy: 0.9000\n",
      "Epoch 13/25\n",
      "360/360 [==============================] - 3s 8ms/step - loss: 0.6450 - accuracy: 0.7083 - val_loss: 0.6264 - val_accuracy: 0.9500\n",
      "Epoch 14/25\n",
      "360/360 [==============================] - 3s 8ms/step - loss: 0.6134 - accuracy: 0.7500 - val_loss: 0.6254 - val_accuracy: 0.6250\n",
      "Epoch 15/25\n",
      "360/360 [==============================] - 3s 9ms/step - loss: 0.5886 - accuracy: 0.7528 - val_loss: 0.6008 - val_accuracy: 0.6250\n",
      "Epoch 16/25\n",
      "360/360 [==============================] - 3s 8ms/step - loss: 0.5422 - accuracy: 0.7972 - val_loss: 0.5423 - val_accuracy: 0.9500\n",
      "Epoch 17/25\n",
      "360/360 [==============================] - 3s 8ms/step - loss: 0.5031 - accuracy: 0.8167 - val_loss: 0.5215 - val_accuracy: 0.8000\n",
      "Epoch 18/25\n",
      "360/360 [==============================] - 3s 8ms/step - loss: 0.4593 - accuracy: 0.8167 - val_loss: 0.4758 - val_accuracy: 0.7750\n",
      "Epoch 19/25\n",
      "360/360 [==============================] - 3s 8ms/step - loss: 0.3980 - accuracy: 0.8611 - val_loss: 0.5578 - val_accuracy: 0.6750\n",
      "Epoch 20/25\n",
      "360/360 [==============================] - 3s 8ms/step - loss: 0.3716 - accuracy: 0.8639 - val_loss: 0.4163 - val_accuracy: 0.8500\n",
      "Epoch 21/25\n",
      "360/360 [==============================] - 3s 8ms/step - loss: 0.3262 - accuracy: 0.8833 - val_loss: 0.3522 - val_accuracy: 0.9750\n",
      "Epoch 22/25\n",
      "360/360 [==============================] - 3s 8ms/step - loss: 0.2908 - accuracy: 0.9194 - val_loss: 0.3512 - val_accuracy: 0.8000\n",
      "Epoch 23/25\n",
      "360/360 [==============================] - 3s 8ms/step - loss: 0.2607 - accuracy: 0.9111 - val_loss: 0.3729 - val_accuracy: 0.7750\n",
      "Epoch 24/25\n",
      "360/360 [==============================] - 3s 8ms/step - loss: 0.2359 - accuracy: 0.9222 - val_loss: 0.2763 - val_accuracy: 0.9250\n",
      "Epoch 25/25\n",
      "360/360 [==============================] - 3s 8ms/step - loss: 0.1983 - accuracy: 0.9361 - val_loss: 0.2667 - val_accuracy: 0.9500\n",
      "[('tp', 100), ('tn', 93), ('fp', 7), ('fn', 0)]\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.1354 - accuracy: 0.9825\n",
      "Scores: [0.13540123403072357, 0.9825000166893005]\n",
      "{'batch_size': 64, 'lr': 1.0, 'activation': 'tanh', 'optimizer': 'Adam', 'epochs': 25, 'arch': [40], 'Score': 0.987500011920929}\n"
     ]
    }
   ],
   "source": [
    "# set the heuristic function to f\n",
    "state[\"f\"] = f\n",
    "# random search for 60 seconds\n",
    "random_results = Search.random_search(state,params,Search.heuristic_function,time=60)\n",
    "# get the random results\n",
    "random_results   = sorted(random_results, key=lambda x: x['Score'])\n",
    "print(random_results[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Score': 0.32749998569488525,\n",
      "  'activation': 'relu',\n",
      "  'arch': [10, 5],\n",
      "  'batch_size': 64,\n",
      "  'epochs': 25,\n",
      "  'lr': 0.0001,\n",
      "  'optimizer': 'Adam'},\n",
      " {'Score': 0.4749999940395355,\n",
      "  'activation': 'sigmoid',\n",
      "  'arch': [80, 40],\n",
      "  'batch_size': 64,\n",
      "  'epochs': 25,\n",
      "  'lr': 1.0,\n",
      "  'optimizer': 'SGD'},\n",
      " {'Score': 0.8575000166893005,\n",
      "  'activation': 'sigmoid',\n",
      "  'arch': [10],\n",
      "  'batch_size': 4,\n",
      "  'epochs': 25,\n",
      "  'lr': 0.001,\n",
      "  'optimizer': 'Adam'},\n",
      " {'Score': 0.9750000238418579,\n",
      "  'activation': 'tanh',\n",
      "  'arch': [40, 40],\n",
      "  'batch_size': 4,\n",
      "  'epochs': 25,\n",
      "  'lr': 0.01,\n",
      "  'optimizer': 'SGD'},\n",
      " {'Score': 0.9825000166893005,\n",
      "  'activation': 'relu',\n",
      "  'arch': [10, 5],\n",
      "  'batch_size': 1,\n",
      "  'epochs': 25,\n",
      "  'lr': 0.01,\n",
      "  'optimizer': 'SGD'},\n",
      " {'Score': 0.987500011920929,\n",
      "  'activation': 'tanh',\n",
      "  'arch': [40],\n",
      "  'batch_size': 64,\n",
      "  'epochs': 25,\n",
      "  'lr': 1.0,\n",
      "  'optimizer': 'Adam'}]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(random_results[-10:]) # print last 10 results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## References\n",
    "\n",
    "* [Theanets Documentation](https://theanets.readthedocs.org/en/stable/)\n",
    "* [A Practical Guide to TrainingRestricted Boltzmann Machines](https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf)\n",
    "* [MLP](http://deeplearning.net/tutorial/mlp.html#mlp)\n",
    "* [Deep Learning Tutorials](http://www.iro.umontreal.ca/~pift6266/H10/notes/deepintro.html)\n",
    "* [Deep Learning Tutorials](http://deeplearning.net/tutorial/)\n",
    "* [Coursera: Hinton's Neural Networks for Machine Learning](https://www.coursera.org/course/neuralnets)\n",
    "* [The Next Generation of Neural Networks](https://www.youtube.com/watch?v=AyzOUbkUf3M)\n",
    "* [Geoffrey Hinton: \"Introduction to Deep Learning & Deep Belief Nets\"](https://www.youtube.com/watch?v=GJdWESd543Y)\n",
    "* Bengio's Deep Learning\n",
    "  [(1)](https://www.youtube.com/watch?v=JuimBuvEWBg)[(2)](https://www.youtube.com/watch?v=Fl-W7_z3w3o)\n",
    "* [Nvidia's Deep Learning tutorials](https://developer.nvidia.com/deep-learning-courses\n",
    ")\n",
    "* [Udacity Deep Learning MOOC](https://www.udacity.com/course/deep-learning--ud730)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
